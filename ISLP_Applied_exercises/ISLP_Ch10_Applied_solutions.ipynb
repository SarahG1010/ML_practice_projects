{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyN4gl7s6+Ik5QOOOFK4rbBf"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["This is the notebook template for ISLP Charpter 9 applied practice."],"metadata":{"id":"uDrmZDtsmZ6g"}},{"cell_type":"markdown","source":["## 10.6.\n","Consider the simple function $R(β) = sin(β) + β/10$.\n","\n","(a) Draw a graph of this function over the range $β ∈ [−6, 6]$.\n"],"metadata":{"id":"2GYYqXzimeJ0"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"3RSCjV48mV_B"},"outputs":[],"source":[]},{"cell_type":"markdown","source":["\n","(b) What is the derivative of this function?\n"],"metadata":{"id":"9OiEvFAemvit"}},{"cell_type":"code","source":[],"metadata":{"id":"jgptZK9CmxVp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["(c) Given $β^0 = 2.3$, run gradient descent to find a local minimum of $R(β)$ using a learning rate of $ρ = 0.1$. Show each of $β^0,β^1,... $in your plot, as well as the final answer.\n"],"metadata":{"id":"igY_1xeSmxoR"}},{"cell_type":"code","source":[],"metadata":{"id":"edLzIFHdm-QX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["(d) Repeat with $β^0 = 1.4$."],"metadata":{"id":"PFzBk1h1m-oB"}},{"cell_type":"code","source":[],"metadata":{"id":"8p8cVzQPnDJC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 10.7.\n","Fit a neural network to the `Default` data. Use a single hidden layer with 10 units, and dropout regularization. Have a look at Labs 10.9.1– 10.9.2 for guidance. Compare the classification performance of your model with that of linear logistic regression."],"metadata":{"id":"6E1x9P09nDgh"}},{"cell_type":"code","source":[],"metadata":{"id":"JXQ965Q9nHyu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##10.8.\n","From your collection of personal photographs, pick 10 images of animals (such as dogs, cats, birds, farm animals, etc.). If the subject does not occupy a reasonable part of the image, then crop the image. Now use a pretrained image classification CNN as in Lab 10.9.4 to predict the class of each of your images, and report the probabilities for the top five predicted classes for each image."],"metadata":{"id":"3iJ39wOWnJlf"}},{"cell_type":"code","source":[],"metadata":{"id":"DTSsTRBrnLdI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##10.9.\n","Fit a lag-5 autoregressive model to the NYSE data, as described in the text and Lab 10.9.6. Refit the model with a 12-level factor repre- senting the month. Does this factor improve the performance of the model?\n"],"metadata":{"id":"IyiqMLBOnL0n"}},{"cell_type":"code","source":[],"metadata":{"id":"_ndWjnwtnbIL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 10.10.\n","In Section 10.9.6, we showed how to fit a linear AR model to the NYSE data using the `LinearRegression()` function. However, we also mentioned that we can “flatten” the short sequences produced for the RNN model in order to fit a linear AR model. Use this latter approach to fit a linear AR model to the NYSE data. Compare the test $R^2$ of this linear AR model to that of the linear AR model that we fit in the lab. What are the advantages/disadvantages of each approach?"],"metadata":{"id":"Szy_ucCvnbfI"}},{"cell_type":"code","source":[],"metadata":{"id":"d8Ox5TQanl_Z"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##10.11.\n","Repeat the previous exercise, but now fit a nonlinear AR model by “flattening” the short sequences produced for the RNN model."],"metadata":{"id":"2xWxfGcwnmck"}},{"cell_type":"code","source":[],"metadata":{"id":"uvAx0BJJnocd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##10.12.\n","Consider the RNN fit to the NYSE data in Section 10.9.6. Modify the code to allow inclusion of the variable day_of_week, and fit the RNN. Compute the test R2."],"metadata":{"id":"9r23ePCvnouz"}},{"cell_type":"code","source":[],"metadata":{"id":"GEoa5Dm9nvxE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##10.13.\n","Repeat the analysis of Lab 10.9.5 on the IMDb data using a similarly structured neural network. We used 16 hidden units at each of two hidden layers. Explore the effect of increasing this to 32 and 64 units per layer, with and without 30% dropout regularization."],"metadata":{"id":"n7WHpGmPnwFO"}},{"cell_type":"code","source":[],"metadata":{"id":"EjEGz1prn1kb"},"execution_count":null,"outputs":[]}]}