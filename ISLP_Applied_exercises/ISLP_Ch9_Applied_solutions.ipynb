{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyO1EBaXz78JCTp52cZ7vJO5"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["This is the notebook template for ISLP Charpter 9 applied practice."],"metadata":{"id":"YkuHuJTokALx"}},{"cell_type":"markdown","source":["##9.4.\n","Generate a simulated two-class data set with 100 observations and two features in which there is a visible but non-linear separation be- tween the two classes. Show that in this setting, a support vector machine with a polynomial kernel (with degree greater than 1) or a radial kernel will outperform a support vector classifier on the train- ing data. Which technique performs best on the test data? Make plots and report training and test error rates in order to back up your assertions."],"metadata":{"id":"gjZemSzxkC_8"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"nVmmm4ATjdTW"},"outputs":[],"source":[]},{"cell_type":"markdown","source":["##9.5.\n","We have seen that we can fit an SVM with a non-linear kernel in order to perform classification using a non-linear decision boundary. We will now see that we can also obtain a non-linear decision boundary by performing logistic regression using non-linear transformations of the features.\n","\n","(a) Generate a data set with n = 500 and p = 2, such that the obser- vations belong to two classes with a quadratic decision boundary between them. For instance, you can do this as follows:"],"metadata":{"id":"ZaIVuKWNkK7S"}},{"cell_type":"code","source":["rng = np.random.default_rng(5)\n","x1 = rng.uniform(size=500) - 0.5\n","x2 = rng.uniform(size=500) - 0.5\n","y = x1**2 - x2**2 > 0\n",""],"metadata":{"id":"mMvSE6cJkSos"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["(b) Plot the observations, colored according to their class labels. Your plot should display $X_1$ on the x-axis, and $X_2$ on the y- axis.\n"],"metadata":{"id":"gM-z6Wd8kW8N"}},{"cell_type":"code","source":[],"metadata":{"id":"srB_0uo_kkOh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["(c) Fit a logistic regression model to the data, using $X_1$ and $X_2$ as predictors.\n"],"metadata":{"id":"OCLifPXtkkpK"}},{"cell_type":"code","source":[],"metadata":{"id":"K3tB_6tnkrSt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["(d) Apply this model to the training data in order to obtain a predicted class label for each training observation. Plot the observations, colored according to the predicted class labels. The decision boundary should be linear.\n"],"metadata":{"id":"X7JGrxSNkrm9"}},{"cell_type":"code","source":[],"metadata":{"id":"tlGQvcg_kvfa"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["(e) Now fit a logistic regression model to the data using non-linear functions of $X_1$ and $X_2$ as predictors (e.g. $X_1^2, X_1 ×X_2, log(X_2)$, and so forth).\n"],"metadata":{"id":"c3x8DDKrkvyb"}},{"cell_type":"code","source":[],"metadata":{"id":"0E3yVXZXlBqO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["(f) Apply this model to the training data in order to obtain a pre- dicted class label for each training observation. Plot the ob- servations, colored according to the $predicted$ class labels. The decision boundary should be obviously non-linear. If it is not, then repeat (a)–(e) until you come up with an example in which the predicted class labels are obviously non-linear.\n"],"metadata":{"id":"uz3HtJ42lCB7"}},{"cell_type":"code","source":[],"metadata":{"id":"Sh0xpeCClIlv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["(g) Fit a support vector classifier to the data with $X_1$ and $X_2$ as predictors. Obtain a class prediction for each training observation. Plot the observations, colored according to the predicted class labels.\n"],"metadata":{"id":"oe6RxRnHlI-8"}},{"cell_type":"code","source":[],"metadata":{"id":"W9yjB_qqlPYE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["(h) Fit a SVM using a non-linear kernel to the data. Obtain a class prediction for each training observation. Plot the observations, colored according to the predicted class labels.\n"],"metadata":{"id":"6M64OZVSlQQ5"}},{"cell_type":"code","source":[],"metadata":{"id":"m3ja2uAolR-8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["(i) Comment on your results."],"metadata":{"id":"uC5C7MrklSSt"}},{"cell_type":"code","source":[],"metadata":{"id":"hXzXZ-M-lS3w"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##9.6.\n","At the end of Section 9.6.1, it is claimed that in the case of data that is just barely linearly separable, a support vector classifier with a small value of C that misclassifies a couple of training observations may perform better on test data than one with a huge value of C that does not misclassify any training observations. You will now investigate this claim.\n","\n","(a) Generate two-class data with p = 2 in such a way that the classes are just barely linearly separable.\n"],"metadata":{"id":"0qlc-uFFlVOx"}},{"cell_type":"code","source":[],"metadata":{"id":"TbWAoae6laa1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["(b) Compute the cross-validation error rates for support vector classifiers with a range of C values. How many training observations are misclassified for each value of C considered, and how does this relate to the cross-validation errors obtained?"],"metadata":{"id":"hUkcPQoZla2q"}},{"cell_type":"code","source":[],"metadata":{"id":"_ub9yMpjld08"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["(c) Generate an appropriate test data set, and compute the test errors corresponding to each of the values of C considered. Which value of C leads to the fewest test errors, and how does this compare to the values of C that yield the fewest training errors and the fewest cross-validation errors?\n","\n"],"metadata":{"id":"S86MH4GMleIg"}},{"cell_type":"code","source":[],"metadata":{"id":"kMytSrO9lfi3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["(d) Discuss your results."],"metadata":{"id":"iFKx-bu2lf3Q"}},{"cell_type":"code","source":[],"metadata":{"id":"5laO4KbOlgoE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##9.7.\n","In this problem, you will use support vector approaches in order to predict whether a given car gets high or low gas mileage based on the Auto data set.\n","\n","(a) Create a binary variable that takes on a 1 for cars with gas mileage above the median, and a 0 for cars with gas mileage below the median.\n"],"metadata":{"id":"3X_q9hZQljXp"}},{"cell_type":"code","source":[],"metadata":{"id":"JzoEbkXGlov3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["(b) Fit a support vector classifier to the data with various values of C, in order to predict whether a car gets high or low gas mileage. Report the cross-validation errors associated with different val- ues of this parameter. Comment on your results. Note you will need to fit the classifier without the gas mileage variable to pro- duce sensible results.\n"],"metadata":{"id":"H0HlgQBFlpCJ"}},{"cell_type":"code","source":[],"metadata":{"id":"kfL-wyrmlqul"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["(c) Now repeat (b), this time using SVMs with radial and polyno- mial basis kernels, with different values of gamma and degree and C. Comment on your results.\n"],"metadata":{"id":"gRXS7bpqlrNM"}},{"cell_type":"code","source":[],"metadata":{"id":"koRAIIxkltBo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["(d) Make some plots to back up your assertions in (b) and (c).\n","\n","$Hint$: In the lab, we used the `plot_svm()`function for fitted SVMs. When `p > 2`, you can use the keyword argument `features` to create plots displaying pairs of variables at a time."],"metadata":{"id":"HkazQHkCltmB"}},{"cell_type":"code","source":[],"metadata":{"id":"PGdFauOwl3js"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##9.8.\n","This problem involves the OJ data set which is part of the ISLP package.\n","\n","(a) Create a training set containing a random sample of 800 observations, and a test set containing the remaining observations.\n","\n"],"metadata":{"id":"VqnYutZ2l4C-"}},{"cell_type":"code","source":[],"metadata":{"id":"r5mjuXOPl8f1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["(b) Fit a support vector classifier to the training data using C = 0.01, with Purchase as the response and the other variables as predictors. How many support points are there?\n"],"metadata":{"id":"ixbTn8Odl8xU"}},{"cell_type":"code","source":[],"metadata":{"id":"_0RD7Fpvl-Q6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["(c) What are the training and test error rates?\n"],"metadata":{"id":"9hfgcCypl-jF"}},{"cell_type":"code","source":[],"metadata":{"id":"6pFEGRd-mANh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["(d) Use cross-validation to select an optimal C. Consider values in\n","the range 0.01 to 10.\n"],"metadata":{"id":"ld0JUirimAdY"}},{"cell_type":"code","source":[],"metadata":{"id":"vCDTILhmmCXq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["(e) Compute the training and test error rates using this new value\n","for C.\n"],"metadata":{"id":"2ogdPNDXmCr6"}},{"cell_type":"code","source":[],"metadata":{"id":"m-Kcz0pdmEIg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["(f) Repeat parts (b) through (e) using a support vector machine\n","with a radial kernel. Use the default value for gamma.\n"],"metadata":{"id":"aMCpGtdJmEZ1"}},{"cell_type":"code","source":[],"metadata":{"id":"AeQHDrNtmFm7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["(g) Repeat parts (b) through (e) using a support vector machine\n","with a polynomial kernel. Set degree = 2.\n"],"metadata":{"id":"GrFv4bcimGDG"}},{"cell_type":"code","source":[],"metadata":{"id":"_s7EpU9nmHYK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["(h) Overall, which approach seems to give the best results on this data?"],"metadata":{"id":"NPfWHNLPmHrD"}},{"cell_type":"code","source":[],"metadata":{"id":"syhfa-X3mInh"},"execution_count":null,"outputs":[]}]}