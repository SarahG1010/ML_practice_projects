{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cLFY3kF1SziY"
   },
   "source": [
    "This is the notebook template for ISLP Charpter 5 applied practice.\n",
    "Reference:\n",
    "* https://www.kaggle.com/code/suugaku/islr-chapter-5-applied-exercises-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "def importlib():\n",
    "    global pd, np, sns, plt, smf, sm, ProbPlot, train_test_split\n",
    "    global LinearRegression, LogisticRegression, accuracy_score, classification_report, confusion_matrix\n",
    "    global LinearDiscriminantAnalysis,QuadraticDiscriminantAnalysis,KNeighborsClassifier,GaussianNB\n",
    "    global Pipeline,cross_val_score, LeaveOneOut, KFold,OneHotEncoder, PolynomialFeatures,ColumnTransformer,resample\n",
    "    # import necessary libraries\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import seaborn as sns\n",
    "    import matplotlib.pyplot as plt\n",
    "    import statsmodels.formula.api as smf\n",
    "    import statsmodels.api as sm\n",
    "    from statsmodels.graphics.gofplots import ProbPlot\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "    from sklearn.metrics import accuracy_score, classification_report,confusion_matrix\n",
    "    from sklearn.discriminant_analysis import LinearDiscriminantAnalysis #linear discriminant analysis\n",
    "    from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis #quadratic discriminant analysis\n",
    "    from sklearn.neighbors import KNeighborsClassifier #K nearest neighbours (KNN)\n",
    "    from sklearn.naive_bayes import GaussianNB\n",
    "    from sklearn.pipeline import Pipeline\n",
    "# Import classes from scikit-learn for logistic regression, least squares linear regression\n",
    "# Import OneHotEncoder and PolynomialFeatures for data pre-processing\n",
    "# Import Pipeline, ColumnTransformer to encapsulate pre-processing heterogenous data and fitting\n",
    "# into a single estimator\n",
    "# Import train_test_split, cross_val_score, LeaveOneOut, KFold for model validation\n",
    "# Import resample for bootstrapping\n",
    "    from sklearn.model_selection import train_test_split, cross_val_score, LeaveOneOut, KFold\n",
    "    from sklearn.preprocessing import OneHotEncoder, PolynomialFeatures\n",
    "    from sklearn.pipeline import Pipeline\n",
    "    from sklearn.compose import ColumnTransformer\n",
    "    from sklearn.utils import resample\n",
    "\n",
    "    \n",
    "    print('done')\n",
    "\n",
    "importlib()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aZm0wpDHTFFT"
   },
   "source": [
    "## 5.5.\n",
    "In Chapter 4, we used logistic regression to predict the probability of `default` using `income` and `balance` on the `Default` data set. We will now estimate the test error of this logistic regression model using the validation set approach. Do not forget to set a random seed before beginning your analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e06xWtadTMQx"
   },
   "source": [
    "(a) Fit a logistic regression model that uses `income` and `balance` to predict `default`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "-f6qIVM9SgDv"
   },
   "outputs": [],
   "source": [
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>default</th>\n",
       "      <th>student</th>\n",
       "      <th>balance</th>\n",
       "      <th>income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>729.526495</td>\n",
       "      <td>44361.625074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>817.180407</td>\n",
       "      <td>12106.134700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  default student     balance        income\n",
       "0      No      No  729.526495  44361.625074\n",
       "1      No     Yes  817.180407  12106.134700"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('Default.csv')\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['default_yes'] = (df['default'] == 'Yes').astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Data columns (total 5 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   default      10000 non-null  object \n",
      " 1   student      10000 non-null  object \n",
      " 2   balance      10000 non-null  float64\n",
      " 3   income       10000 non-null  float64\n",
      " 4   default_yes  10000 non-null  int64  \n",
      "dtypes: float64(2), int64(1), object(2)\n",
      "memory usage: 390.8+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are looking for LogisticRegression without regularization. In sklearn this is not implemented, but we can use l2 regularization and set C, the inverese strenght, to a very high number, effectively removing the regularization. We also compare the coefficients to the ones obtained from statsmodel LogisticRegression which has no regularization, and we verify that the coefficient estimates match.\n",
    "\n",
    "Another parameter we have to consider is the tolerance. In this case, the default 1e-4 is not enough to reach convergence, so we increased it until it did.\n",
    "\n",
    "Below we that the coefficients obtained with sklearn agree with those from statsmodels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_cols = ['income','balance']\n",
    "y_col = ['default_yes']\n",
    "X = df[x_cols]\n",
    "y = df[y_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [[2.08089921e-05 5.64710291e-03]]\n",
      "Intercept: [-11.54046839]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "model1 = LogisticRegression(C=1e6, tol = 1e-6)\n",
    "res1 = model1.fit(X,y)\n",
    "\n",
    "# print summary\n",
    "# Access the coefficients\n",
    "coefficients = model1.coef_\n",
    "intercept = model1.intercept_\n",
    "\n",
    "print(\"Coefficients:\", coefficients)\n",
    "print(\"Intercept:\", intercept)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.078948\n",
      "         Iterations 10\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:            default_yes   No. Observations:                10000\n",
      "Model:                          Logit   Df Residuals:                     9997\n",
      "Method:                           MLE   Df Model:                            2\n",
      "Date:                Mon, 06 May 2024   Pseudo R-squ.:                  0.4594\n",
      "Time:                        13:31:01   Log-Likelihood:                -789.48\n",
      "converged:                       True   LL-Null:                       -1460.3\n",
      "Covariance Type:            nonrobust   LLR p-value:                4.541e-292\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept    -11.5405      0.435    -26.544      0.000     -12.393     -10.688\n",
      "income      2.081e-05   4.99e-06      4.174      0.000     1.1e-05    3.06e-05\n",
      "balance        0.0056      0.000     24.835      0.000       0.005       0.006\n",
      "==============================================================================\n",
      "\n",
      "Possibly complete quasi-separation: A fraction 0.14 of observations can be\n",
      "perfectly predicted. This might indicate that there is complete\n",
      "quasi-separation. In this case some parameters will not be identified.\n"
     ]
    }
   ],
   "source": [
    "model2 = smf.logit(formula = 'default_yes ~ income + balance', data = df)\n",
    "res2 = model2.fit()\n",
    "print(res2.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cTe1QvfnTZwT"
   },
   "source": [
    "(b) Using the validation set approach, estimate the test error of this model. In order to do this, you must perform the following steps:\n",
    "\n",
    "i. Split the sample set into a training set and a validation set.\n",
    "\n",
    "ii. Fit a multiple logistic regression model using only the training observations.\n",
    "\n",
    "iii. Obtain a prediction of default status for each individual in the validation set by computing the posterior probability of default for that individual, and classifying the individual to the `default` category if the posterior probability is greater than 0.5.\n",
    "\n",
    "iv. Compute the validation set error, which is the fraction of the observations in the validation set that are misclassified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            income      balance\n",
      "5341  33832.489629  1024.326377\n",
      "7483  39914.919048   520.654613\n",
      "      default_yes\n",
      "5341            0\n",
      "7483            0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y)\n",
    "print(X_train.head(2))\n",
    "print(y_train.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>default_yes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9203</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7827</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      default_yes\n",
       "9203            0\n",
       "7827            1"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "oLABS76qTmi-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2411    8]\n",
      " [  51   30]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "res10 = model1.fit(X_train, y_train)\n",
    "y_pred = res10.predict(X_test)\n",
    "conf_m = confusion_matrix(y_test,y_pred)\n",
    "print(conf_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2411</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>51</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0   1\n",
       "0  2411   8\n",
       "1    51  30"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat = pd.DataFrame(confusion_matrix(y_test,y_pred), columns = [\"0\", \"1\"], index = [\"0\", \"1\"])\n",
    "mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wxMVnlArTm6G"
   },
   "source": [
    "(c) Repeat the process in (b) three times, using three different splits of the observations into a training set and a validation set. Comment on the results obtained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "uesxR9zXTqyi"
   },
   "outputs": [],
   "source": [
    "## Create 3 different splits\n",
    "# First split\n",
    "X_train1, X_test1, y_train1, y_test1 = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Second split\n",
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(X, y, test_size=0.2, random_state=100)\n",
    "\n",
    "# Third split\n",
    "X_train3, X_test3, y_train3, y_test3 = train_test_split(X, y, test_size=0.2, random_state=2022)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>default_yes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6252</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4684</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      default_yes\n",
       "6252            0\n",
       "4684            0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test1.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>default_yes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8018</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9225</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      default_yes\n",
       "8018            0\n",
       "9225            0"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test2.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>default_yes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6487</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8785</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      default_yes\n",
       "6487            0\n",
       "8785            0"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test3.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1931</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0  1\n",
       "0  1931  0\n",
       "1    69  0"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res11 = model1.fit(X_train1, y_train1)\n",
    "y_pred = res11.predict(X_test1)\n",
    "mat = pd.DataFrame(confusion_matrix(y_test1,y_pred), columns = [\"0\", \"1\"], index = [\"0\", \"1\"])\n",
    "mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1935</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>65</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0  1\n",
       "0  1935  0\n",
       "1    65  0"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res12 = model1.fit(X_train2, y_train2)\n",
    "y_pred = res12.predict(X_test2)\n",
    "mat = pd.DataFrame(confusion_matrix(y_test2,y_pred), columns = [\"0\", \"1\"], index = [\"0\", \"1\"])\n",
    "mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1938</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0  1\n",
       "0  1938  0\n",
       "1    62  0"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res13 = model1.fit(X_train3, y_train3)\n",
    "y_pred = res12.predict(X_test3)\n",
    "mat = pd.DataFrame(confusion_matrix(y_test3,y_pred), columns = [\"0\", \"1\"], index = [\"0\", \"1\"])\n",
    "mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A little bit weired results above, try out another method\n",
    "### It is right!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.034499999999999975"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                   test_size = 0.2, random_state = 42)\n",
    "clf = LogisticRegression(penalty = \"none\", solver = \"lbfgs\")\n",
    "clf.fit(X_train, y_train)\n",
    "1 - clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1931</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0  1\n",
       "0  1931  0\n",
       "1    69  0"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = clf.predict(X_test)\n",
    "mat = pd.DataFrame(confusion_matrix(y_test,y_pred), columns = [\"0\", \"1\"], index = [\"0\", \"1\"])\n",
    "mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.03249999999999997"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                   test_size = 0.2, random_state = 100)\n",
    "clf = LogisticRegression(penalty = \"none\", solver = \"lbfgs\")\n",
    "clf.fit(X_train, y_train)\n",
    "1 - clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1935</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>65</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0  1\n",
       "0  1935  0\n",
       "1    65  0"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = clf.predict(X_test)\n",
    "mat = pd.DataFrame(confusion_matrix(y_test,y_pred), columns = [\"0\", \"1\"], index = [\"0\", \"1\"])\n",
    "mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.031000000000000028"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                   test_size = 0.2, random_state = 2022)\n",
    "clf = LogisticRegression(penalty = \"none\", solver = \"lbfgs\")\n",
    "clf.fit(X_train, y_train)\n",
    "1 - clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1938</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0  1\n",
       "0  1938  0\n",
       "1    62  0"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat = pd.DataFrame(confusion_matrix(y_test,y_pred), columns = [\"0\", \"1\"], index = [\"0\", \"1\"])\n",
    "mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.03200000000000003"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                   test_size = 0.25, random_state = 456)\n",
    "clf = LogisticRegression(penalty = \"none\", solver = \"lbfgs\")\n",
    "clf.fit(X_train, y_train)\n",
    "1 - clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2420</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>79</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0  1\n",
       "0  2420  1\n",
       "1    79  0"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = clf.predict(X_test)\n",
    "mat = pd.DataFrame(confusion_matrix(y_test,y_pred), columns = [\"0\", \"1\"], index = [\"0\", \"1\"])\n",
    "mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.034399999999999986"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                   test_size = 0.25, random_state = 789)\n",
    "clf = LogisticRegression(penalty = \"none\", solver = \"lbfgs\")\n",
    "clf.fit(X_train, y_train)\n",
    "1 - clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2394</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>76</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0   1\n",
       "0  2394  10\n",
       "1    76  20"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = clf.predict(X_test)\n",
    "mat = pd.DataFrame(confusion_matrix(y_test,y_pred), columns = [\"0\", \"1\"], index = [\"0\", \"1\"])\n",
    "mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.031200000000000006"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                   test_size = 0.25, random_state = 314159)\n",
    "clf = LogisticRegression(penalty = \"none\", solver = \"lbfgs\")\n",
    "clf.fit(X_train, y_train)\n",
    "1 - clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2422</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>78</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0  1\n",
       "0  2422  0\n",
       "1    78  0"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = clf.predict(X_test)\n",
    "mat = pd.DataFrame(confusion_matrix(y_test,y_pred), columns = [\"0\", \"1\"], index = [\"0\", \"1\"])\n",
    "mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0333"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(df[\"default\"] != \"No\").mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comment:\n",
    "Using three different 75-25 splits of the data, our validation set error remained fairly consistent. The average error between this part and Part 2 was 0.0315, and errors were all within about 20% of each other. We also note, however, that these test errors are all fairly close to the error one would achieve with a naive strategy of just saying that nobody will default. The average of the test errors is only about 5% less than the error from using the naive strategy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_ulWhIRETrI5"
   },
   "source": [
    "(d) Now consider a logistic regression model that predicts the probability of `default` using `income, balance`, and a dummy variable for `student`. Estimate the test error for this model using the validation set approach. Comment on whether or not including a dummy variable for `student` leads to a reduction in the test error rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "id": "T0jqSvlST4tM"
   },
   "outputs": [],
   "source": [
    "np.random.seed(312)\n",
    "with_student = {}\n",
    "without_student = {}\n",
    "\n",
    "# Create two classifier pipelines\n",
    "# with_student takes the student variable and encodes it using one hot encoding, passes through income, balance\n",
    "# without_student drops the student variable and only passes through income and balance\n",
    "categorical_features = [\"student\"]\n",
    "categorical_transformer = Pipeline([(\"onehot\", OneHotEncoder(drop = \"first\"))])\n",
    "numerical_features = [\"income\", \"balance\"]\n",
    "with_student_preprocessor = ColumnTransformer([(\"cat\", categorical_transformer, categorical_features),\n",
    "                                 (\"num\", \"passthrough\", numerical_features)])\n",
    "with_student_clf = Pipeline([(\"preprocessor\", with_student_preprocessor), \n",
    "                (\"classifier\", LogisticRegression(penalty = \"none\", solver = \"lbfgs\"))])\n",
    "without_student_preprocessor = ColumnTransformer([(\"num\", \"passthrough\", numerical_features)])\n",
    "without_student_clf = Pipeline([(\"preprocessor\", without_student_preprocessor), \n",
    "                (\"classifier\", LogisticRegression(penalty = \"none\", solver = \"lbfgs\"))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>with_student</th>\n",
       "      <th>without_student</th>\n",
       "      <th>difference</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0316</td>\n",
       "      <td>0.0316</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0336</td>\n",
       "      <td>0.0316</td>\n",
       "      <td>0.0020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0308</td>\n",
       "      <td>0.0308</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0324</td>\n",
       "      <td>0.0256</td>\n",
       "      <td>0.0068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0268</td>\n",
       "      <td>0.0264</td>\n",
       "      <td>0.0004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0332</td>\n",
       "      <td>0.0332</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0328</td>\n",
       "      <td>0.0284</td>\n",
       "      <td>0.0044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.0348</td>\n",
       "      <td>0.0304</td>\n",
       "      <td>0.0044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.0344</td>\n",
       "      <td>0.0280</td>\n",
       "      <td>0.0064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.0300</td>\n",
       "      <td>0.0272</td>\n",
       "      <td>0.0028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.0304</td>\n",
       "      <td>0.0304</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.0280</td>\n",
       "      <td>0.0312</td>\n",
       "      <td>-0.0032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.0400</td>\n",
       "      <td>0.0384</td>\n",
       "      <td>0.0016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.0384</td>\n",
       "      <td>0.0288</td>\n",
       "      <td>0.0096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.0376</td>\n",
       "      <td>0.0300</td>\n",
       "      <td>0.0076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.0384</td>\n",
       "      <td>0.0252</td>\n",
       "      <td>0.0132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.0336</td>\n",
       "      <td>0.0272</td>\n",
       "      <td>0.0064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.0288</td>\n",
       "      <td>0.0292</td>\n",
       "      <td>-0.0004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.0360</td>\n",
       "      <td>0.0300</td>\n",
       "      <td>0.0060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.0300</td>\n",
       "      <td>0.0252</td>\n",
       "      <td>0.0048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.0360</td>\n",
       "      <td>0.0292</td>\n",
       "      <td>0.0068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.0352</td>\n",
       "      <td>0.0280</td>\n",
       "      <td>0.0072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.0284</td>\n",
       "      <td>0.0256</td>\n",
       "      <td>0.0028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.0268</td>\n",
       "      <td>0.0364</td>\n",
       "      <td>-0.0096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.0348</td>\n",
       "      <td>0.0276</td>\n",
       "      <td>0.0072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.0260</td>\n",
       "      <td>0.0248</td>\n",
       "      <td>0.0012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.0396</td>\n",
       "      <td>0.0300</td>\n",
       "      <td>0.0096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.0348</td>\n",
       "      <td>0.0364</td>\n",
       "      <td>-0.0016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.0308</td>\n",
       "      <td>0.0264</td>\n",
       "      <td>0.0044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.0324</td>\n",
       "      <td>0.0348</td>\n",
       "      <td>-0.0024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.0364</td>\n",
       "      <td>0.0272</td>\n",
       "      <td>0.0092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.0296</td>\n",
       "      <td>0.0236</td>\n",
       "      <td>0.0060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.0328</td>\n",
       "      <td>0.0344</td>\n",
       "      <td>-0.0016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.0360</td>\n",
       "      <td>0.0280</td>\n",
       "      <td>0.0080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.0300</td>\n",
       "      <td>0.0208</td>\n",
       "      <td>0.0092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.0308</td>\n",
       "      <td>0.0308</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.0300</td>\n",
       "      <td>0.0296</td>\n",
       "      <td>0.0004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.0320</td>\n",
       "      <td>0.0320</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.0328</td>\n",
       "      <td>0.0268</td>\n",
       "      <td>0.0060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.0316</td>\n",
       "      <td>0.0312</td>\n",
       "      <td>0.0004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.0308</td>\n",
       "      <td>0.0308</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.0368</td>\n",
       "      <td>0.0368</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.0368</td>\n",
       "      <td>0.0320</td>\n",
       "      <td>0.0048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.0372</td>\n",
       "      <td>0.0372</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.0228</td>\n",
       "      <td>0.0232</td>\n",
       "      <td>-0.0004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.0320</td>\n",
       "      <td>0.0320</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.0308</td>\n",
       "      <td>0.0308</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.0304</td>\n",
       "      <td>0.0244</td>\n",
       "      <td>0.0060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0.0356</td>\n",
       "      <td>0.0368</td>\n",
       "      <td>-0.0012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0.0376</td>\n",
       "      <td>0.0376</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    with_student  without_student  difference\n",
       "0         0.0316           0.0316      0.0000\n",
       "1         0.0336           0.0316      0.0020\n",
       "2         0.0308           0.0308      0.0000\n",
       "3         0.0324           0.0256      0.0068\n",
       "4         0.0268           0.0264      0.0004\n",
       "5         0.0332           0.0332      0.0000\n",
       "6         0.0328           0.0284      0.0044\n",
       "7         0.0348           0.0304      0.0044\n",
       "8         0.0344           0.0280      0.0064\n",
       "9         0.0300           0.0272      0.0028\n",
       "10        0.0304           0.0304      0.0000\n",
       "11        0.0280           0.0312     -0.0032\n",
       "12        0.0400           0.0384      0.0016\n",
       "13        0.0384           0.0288      0.0096\n",
       "14        0.0376           0.0300      0.0076\n",
       "15        0.0384           0.0252      0.0132\n",
       "16        0.0336           0.0272      0.0064\n",
       "17        0.0288           0.0292     -0.0004\n",
       "18        0.0360           0.0300      0.0060\n",
       "19        0.0300           0.0252      0.0048\n",
       "20        0.0360           0.0292      0.0068\n",
       "21        0.0352           0.0280      0.0072\n",
       "22        0.0284           0.0256      0.0028\n",
       "23        0.0268           0.0364     -0.0096\n",
       "24        0.0348           0.0276      0.0072\n",
       "25        0.0260           0.0248      0.0012\n",
       "26        0.0396           0.0300      0.0096\n",
       "27        0.0348           0.0364     -0.0016\n",
       "28        0.0308           0.0264      0.0044\n",
       "29        0.0324           0.0348     -0.0024\n",
       "30        0.0364           0.0272      0.0092\n",
       "31        0.0296           0.0236      0.0060\n",
       "32        0.0328           0.0344     -0.0016\n",
       "33        0.0360           0.0280      0.0080\n",
       "34        0.0300           0.0208      0.0092\n",
       "35        0.0308           0.0308      0.0000\n",
       "36        0.0300           0.0296      0.0004\n",
       "37        0.0320           0.0320      0.0000\n",
       "38        0.0328           0.0268      0.0060\n",
       "39        0.0316           0.0312      0.0004\n",
       "40        0.0308           0.0308      0.0000\n",
       "41        0.0368           0.0368      0.0000\n",
       "42        0.0368           0.0320      0.0048\n",
       "43        0.0372           0.0372      0.0000\n",
       "44        0.0228           0.0232     -0.0004\n",
       "45        0.0320           0.0320      0.0000\n",
       "46        0.0308           0.0308      0.0000\n",
       "47        0.0304           0.0244      0.0060\n",
       "48        0.0356           0.0368     -0.0012\n",
       "49        0.0376           0.0376      0.0000"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loop through 50 train-test splits to compute average difference in error rate\n",
    "for i in range(50):\n",
    "    # Split the data in to training and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(df, df[\"default\"], test_size = 0.25)\n",
    "    # Fit classifier which includes student variable and compute validation set error\n",
    "    with_student_clf.fit(X_train, y_train);\n",
    "    with_student[i] = 1 - with_student_clf.score(X_test, y_test);\n",
    "    # Fit classifier which excludes student variable and compute validation set error\n",
    "    without_student_clf.fit(X_train, y_train);\n",
    "    without_student[i] = 1 - without_student_clf.score(X_test, y_test);\n",
    "errors = pd.DataFrame({\"with_student\": with_student, \"without_student\": without_student})\n",
    "errors[\"difference\"] = errors[\"with_student\"] - errors[\"without_student\"]\n",
    "errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.002904000000000009"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "errors[\"difference\"].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looping through 50 train-test splits and comparing the error rates between the logistic regression model predicting default using income, balance, and student and the logistic regression model predicting default using just income and balance for each split, we see that, on average, including a dummy variable for student does not lead to a reduction in the test error rate. In fact, on average it resulted in a very slight increase in the test error rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Umoo2-PsT94f"
   },
   "source": [
    "### 5.6.\n",
    "We continue to consider the use of a logistic regression model to predict the probability of `default` using `income` and `balance` on the `Default` data set. In particular, we will now compute estimates for the standard errors of the `income` and `balance` logistic regression coefficients in two different ways: (1) using the bootstrap, and (2) using the standard formula for computing the standard errors in the `sm.GLM()` function. Do not forget to set a random seed before beginning your analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yVrJZqyWUdm7"
   },
   "source": [
    "(a) Using the `summarize()` and `sm.GLM()` functions, determine the estimated standard errors for the coefficients associated with `income` and `balance` in a multiple logistic regression model that uses both predictors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "id": "6P-a7RqDUjCS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.078948\n",
      "         Iterations 10\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Logit Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>        <td>default</td>     <th>  No. Observations:  </th>   <td> 10000</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                 <td>Logit</td>      <th>  Df Residuals:      </th>   <td>  9997</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>                 <td>MLE</td>       <th>  Df Model:          </th>   <td>     2</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>            <td>Mon, 06 May 2024</td> <th>  Pseudo R-squ.:     </th>   <td>0.4594</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                <td>14:39:46</td>     <th>  Log-Likelihood:    </th>  <td> -789.48</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>converged:</th>             <td>True</td>       <th>  LL-Null:           </th>  <td> -1460.3</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>     <td>nonrobust</td>    <th>  LLR p-value:       </th> <td>4.541e-292</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "     <td></td>        <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>   <td>  -11.5405</td> <td>    0.435</td> <td>  -26.544</td> <td> 0.000</td> <td>  -12.393</td> <td>  -10.688</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>income</th>  <td> 2.081e-05</td> <td> 4.99e-06</td> <td>    4.174</td> <td> 0.000</td> <td>  1.1e-05</td> <td> 3.06e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>balance</th> <td>    0.0056</td> <td>    0.000</td> <td>   24.835</td> <td> 0.000</td> <td>    0.005</td> <td>    0.006</td>\n",
       "</tr>\n",
       "</table><br/><br/>Possibly complete quasi-separation: A fraction 0.14 of observations can be<br/>perfectly predicted. This might indicate that there is complete<br/>quasi-separation. In this case some parameters will not be identified."
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Dep. Variable:}   &     default      & \\textbf{  No. Observations:  } &    10000    \\\\\n",
       "\\textbf{Model:}           &      Logit       & \\textbf{  Df Residuals:      } &     9997    \\\\\n",
       "\\textbf{Method:}          &       MLE        & \\textbf{  Df Model:          } &        2    \\\\\n",
       "\\textbf{Date:}            & Mon, 06 May 2024 & \\textbf{  Pseudo R-squ.:     } &   0.4594    \\\\\n",
       "\\textbf{Time:}            &     14:39:46     & \\textbf{  Log-Likelihood:    } &   -789.48   \\\\\n",
       "\\textbf{converged:}       &       True       & \\textbf{  LL-Null:           } &   -1460.3   \\\\\n",
       "\\textbf{Covariance Type:} &    nonrobust     & \\textbf{  LLR p-value:       } & 4.541e-292  \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lcccccc}\n",
       "                 & \\textbf{coef} & \\textbf{std err} & \\textbf{z} & \\textbf{P$> |$z$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n",
       "\\midrule\n",
       "\\textbf{const}   &     -11.5405  &        0.435     &   -26.544  &         0.000        &      -12.393    &      -10.688     \\\\\n",
       "\\textbf{income}  &    2.081e-05  &     4.99e-06     &     4.174  &         0.000        &      1.1e-05    &     3.06e-05     \\\\\n",
       "\\textbf{balance} &       0.0056  &        0.000     &    24.835  &         0.000        &        0.005    &        0.006     \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "%\\caption{Logit Regression Results}\n",
       "\\end{center}\n",
       "\n",
       "Possibly complete quasi-separation: A fraction 0.14 of observations can be \\newline\n",
       " perfectly predicted. This might indicate that there is complete \\newline\n",
       " quasi-separation. In this case some parameters will not be identified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                           Logit Regression Results                           \n",
       "==============================================================================\n",
       "Dep. Variable:                default   No. Observations:                10000\n",
       "Model:                          Logit   Df Residuals:                     9997\n",
       "Method:                           MLE   Df Model:                            2\n",
       "Date:                Mon, 06 May 2024   Pseudo R-squ.:                  0.4594\n",
       "Time:                        14:39:46   Log-Likelihood:                -789.48\n",
       "converged:                       True   LL-Null:                       -1460.3\n",
       "Covariance Type:            nonrobust   LLR p-value:                4.541e-292\n",
       "==============================================================================\n",
       "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const        -11.5405      0.435    -26.544      0.000     -12.393     -10.688\n",
       "income      2.081e-05   4.99e-06      4.174      0.000     1.1e-05    3.06e-05\n",
       "balance        0.0056      0.000     24.835      0.000       0.005       0.006\n",
       "==============================================================================\n",
       "\n",
       "Possibly complete quasi-separation: A fraction 0.14 of observations can be\n",
       "perfectly predicted. This might indicate that there is complete\n",
       "quasi-separation. In this case some parameters will not be identified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using the Logit class from StatsModels\n",
    "# First encode response numerically\n",
    "y = (df[\"default\"] == \"Yes\").astype(int)\n",
    "X = sm.add_constant(df[[\"income\", \"balance\"]])\n",
    "mod = sm.Logit(y,X)\n",
    "res = mod.fit()\n",
    "res.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.9852454617530595e-06"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.bse[\"income\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00022738138476844987"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.bse[\"balance\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        Generalized Linear Model Regression Results                        \n",
      "===========================================================================================\n",
      "Dep. Variable:     ['default[No]', 'default[Yes]']   No. Observations:                10000\n",
      "Model:                                         GLM   Df Residuals:                     9997\n",
      "Model Family:                             Binomial   Df Model:                            2\n",
      "Link Function:                               Logit   Scale:                          1.0000\n",
      "Method:                                       IRLS   Log-Likelihood:                -789.48\n",
      "Date:                             Mon, 06 May 2024   Deviance:                       1579.0\n",
      "Time:                                     14:51:48   Pearson chi2:                 6.95e+03\n",
      "No. Iterations:                                  9   Pseudo R-squ. (CS):             0.1256\n",
      "Covariance Type:                         nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept     11.5405      0.435     26.544      0.000      10.688      12.393\n",
      "income     -2.081e-05   4.99e-06     -4.174      0.000   -3.06e-05    -1.1e-05\n",
      "balance       -0.0056      0.000    -24.835      0.000      -0.006      -0.005\n",
      "==============================================================================\n"
     ]
    }
   ],
   "source": [
    "# use sm.GLM()\n",
    "#using generalized linear models with statsmodel\n",
    "#see the wikipedia reference to understand why family is binomial\n",
    "mod1 = smf.glm(formula='default ~ income + balance', data=df, family=sm.families.Binomial()).fit() #create & fit model\n",
    "print(mod1.summary()) #show results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GKmd2FBUUkym"
   },
   "source": [
    "(b) Write a function, `boot_fn()`, that takes as input the `Default` data set as well as an index of the observations, and that outputs the coefficient estimates for `income` and `balance` in the multiple logistic regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "id": "_R4cj07pUqSc"
   },
   "outputs": [],
   "source": [
    "def boot_fn(df,rd_index):\n",
    "    '''\n",
    "    Sampling observations from a dataframe, and return the logistic regression coef\n",
    "\n",
    "    Parameters\n",
    "    ------------\n",
    "    df : pandas dataframe(Data to be resampled)\n",
    "\n",
    "    rd_index: random generated array (size < df size)\n",
    "\n",
    "    Returns\n",
    "    ------------\n",
    "    coefs : coefs from logistic regression\n",
    "    '''\n",
    "    #assign default size if non-specified\n",
    "\n",
    "    #create random integers to use as indices for bootstrap sample based on original data\n",
    "    bootSample = df.iloc[rd_index]\n",
    "    mod = smf.glm(formula='default ~ income + balance', data=df, family=sm.families.Binomial()).fit()\n",
    "    coef_income = mod.params[1]\n",
    "    coef_balance = mod.params[2]\n",
    "    return [coef_income, coef_balance]\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# another option to get the random index(seems wrong)\n",
    "#bootSample_size = 500\n",
    "#bootSample_i = np.random.rand(bootSample_size)\n",
    "#bootSample_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bootSample_i = (np.random.rand(bootSample_size)*bootSample_size).astype(int)\n",
    "#bootSample_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bootSample_arr = np.array(bootSample_i)\n",
    "#bootSample_arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1QdGcuyUUqpu"
   },
   "source": [
    "(c) Following the bootstrap example in the lab, use your `boot_fn()` function to estimate the standard errors of the logistic regression coefficients for `income` and `balance`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one option to get the random idx\n",
    "seed = 0\n",
    "rng = np.random.default_rng(seed) \n",
    "#running model for bootstrapped samples\n",
    "\n",
    "bootSample_size = len(df)\n",
    "idx = rng.choice(df.index,\n",
    "                         bootSample_size,\n",
    "                         replace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0   -0.000021\n",
      "1   -0.005647\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "coefficients = [] #variable initialization\n",
    "n = 100 #number of bootstrapped samples\n",
    "\n",
    "for i in range(0,n):\n",
    "    \n",
    "    coef_i = boot_fn(df,idx) #determining coefficients for specific bootstrapped sample\n",
    "    coefficients.append(coef_i) #saving coefficients value\n",
    "\n",
    "print(pd.DataFrame(coefficients).mean()) #print average of coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    2.724160e-20\n",
      "1    5.230388e-18\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(pd.DataFrame(coefficients).std()) #print s.e. of coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seems the bootstrap std is rather small, try another code from\n",
    "https://botlnec.github.io/islp/sols/chapter5/exercise6/\n",
    "### !!!seems boot_se() is not the same as df.std() (see labs for correction)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def boot_fn(default):\n",
    "    mod1 = smf.glm(formula='default ~ income + balance', data=df, family=sm.families.Binomial()).fit()\n",
    "    coef_income = mod1.params[1]\n",
    "    coef_balance = mod1.params[2]\n",
    "    return [coef_income, coef_balance]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bootstrap function\n",
    "def boot(X, bootSample_size=None):\n",
    "    '''\n",
    "    Sampling observations from a dataframe\n",
    "\n",
    "    Parameters\n",
    "    ------------\n",
    "    X : pandas dataframe\n",
    "        Data to be resampled\n",
    "\n",
    "    bootSample_size: int, optional\n",
    "        Dimension of the bootstrapped samples\n",
    "\n",
    "    Returns\n",
    "    ------------\n",
    "    bootSample_X : pandas dataframe\n",
    "        Resampled data\n",
    "\n",
    "    Examples\n",
    "    ----------\n",
    "    To resample data from the X dataframe:\n",
    "        >> boot(X)\n",
    "    The resampled data will have length equal to len(X).\n",
    "\n",
    "    To resample data from the X dataframe in order to have length 5:\n",
    "        >> boot(X,5)\n",
    "\n",
    "    References\n",
    "    ------------\n",
    "    http://nbviewer.jupyter.org/gist/aflaxman/6871948\n",
    "\n",
    "    '''\n",
    "\n",
    "    #assign default size if non-specified\n",
    "    if bootSample_size == None:\n",
    "        bootSample_size = len(X)\n",
    "\n",
    "    #create random integers to use as indices for bootstrap sample based on original data\n",
    "    bootSample_i = (np.random.rand(bootSample_size)*len(X)).astype(int)\n",
    "    bootSample_i = np.array(bootSample_i)\n",
    "    bootSample_X = X.iloc[bootSample_i]\n",
    "\n",
    "    return bootSample_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0   -0.000021\n",
      "1   -0.005647\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#running model for bootstrapped samples\n",
    "coefficients = [] #variable initialization\n",
    "n = 100 #number of bootstrapped samples\n",
    "\n",
    "for i in range(0,n):\n",
    "    coef_i = boot_fn(boot(df)) #determining coefficients for specific bootstrapped sample\n",
    "    coefficients.append(coef_i) #saving coefficients value\n",
    "\n",
    "print(pd.DataFrame(coefficients).mean()) #print average of coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    2.724160e-20\n",
      "1    5.230388e-18\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(pd.DataFrame(coefficients).std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Standard Error (boot_se)**: The standard error of a coefficient estimate provides a measure of the uncertainty or variability in the estimated coefficient due to sampling variability. In bootstrapping, the standard error is typically estimated by repeatedly resampling the dataset with replacement, fitting the model to each resampled dataset, and calculating the standard deviation of the coefficient estimates across the bootstrap samples. This captures the variability in coefficient estimates that arises from sampling variation in the data.\n",
    "* **Standard Deviation (df.std)**: On the other hand, the standard deviation of the coefficients (as calculated from the dataframe df) represents the variability of the coefficients across different observations in the dataset. It measures the spread of the coefficients around their mean value within the original dataset.\n",
    "\n",
    "* **These two measures capture different types of variability**:\n",
    "\n",
    "* The standard error captures the variability in coefficient estimates due to sampling variability.\n",
    "* The standard deviation captures the variability in coefficient estimates across different observations in the dataset.\n",
    "\n",
    "### Corrected code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def boot_fn(default):\n",
    "    mod1 = smf.glm(formula='default ~ income + balance', data=df, family=sm.families.Binomial()).fit()\n",
    "    coef_income = mod1.params[1]\n",
    "    coef_balance = mod1.params[2]\n",
    "    std_income = mod1.bse[1]\n",
    "    std_balance = mod1.bse[2]\n",
    "    return [coef_income, coef_balance,std_income,std_balance]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0   -0.000021\n",
      "1   -0.005647\n",
      "2    0.000005\n",
      "3    0.000227\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "coefficients = [] #variable initialization\n",
    "n = 100 #number of bootstrapped samples\n",
    "\n",
    "for i in range(0,n):\n",
    "    coef_i = boot_fn(boot(df)) #determining coefficients for specific bootstrapped sample\n",
    "    coefficients.append(coef_i) #saving coefficients value\n",
    "\n",
    "#print average of coefficients and se for coffcients\n",
    "# 0: income_coef\n",
    "# 1: balance_coef\n",
    "# 3: income_std\n",
    "# 4: balance_std\n",
    "print(pd.DataFrame(coefficients).mean()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### use sklearn \n",
    "#### Question: will each randomly selecting samples to be the same set each time?\n",
    "**Answer**:\n",
    "No, each time you perform bootstrap resampling with a random seed, the randomly selected samples will not be the same set each time. \n",
    "\n",
    "Although setting a random seed ensures reproducibility by making the random number generation deterministic, the bootstrapping process still involves random sampling with replacement. This means that even though the same seed is used, the resampling process will result in different bootstrap samples each time due to the random selection of observations.\n",
    "\n",
    "Here's a breakdown of how it works:\n",
    "\n",
    "1. Setting the random seed ensures that the random number generator produces the same sequence of pseudo-random numbers each time.\n",
    "2. However, when you perform bootstrap resampling, you're randomly selecting samples with replacement from your original dataset.\n",
    "3. Since the random selection is based on the sequence of pseudo-random numbers generated by the random number generator, each bootstrap sample will be different, even though the same seed is used.\n",
    "\n",
    "So, while the random seed ensures that the resampling process is reproducible, it does not mean that the same set of observations will be selected each time. Instead, it ensures that the random sampling process follows the same sequence of pseudo-random numbers, resulting in consistent and reproducible results across different runs of the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of bootstrap means: 5.5459\n",
      "Standard error of bootstrap means: 0.02798045918308161\n"
     ]
    }
   ],
   "source": [
    "## An example ##\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Set the random seed for reproducibility\n",
    "np.random.seed(17)\n",
    "\n",
    "# Sample data\n",
    "original_data = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n",
    "\n",
    "# Number of bootstrap iterations\n",
    "num_bootstrap_iterations = 1000\n",
    "\n",
    "# Bootstrap resampling\n",
    "bootstrap_samples = []\n",
    "for _ in range(num_bootstrap_iterations):\n",
    "    # Generate random indices with replacement\n",
    "    indices = np.random.choice(original_data, size=len(original_data), replace=True)\n",
    "    # Get the bootstrap sample\n",
    "    bootstrap_sample = original_data[indices-1]\n",
    "    # Append the bootstrap sample to the list\n",
    "    bootstrap_samples.append(bootstrap_sample)\n",
    "\n",
    "# Conduct analysis on the resampled data (e.g., compute statistics)\n",
    "# For example, calculate the mean of each bootstrap sample\n",
    "bootstrap_means = [sample.mean() for sample in bootstrap_samples]\n",
    "\n",
    "# Compute the mean and standard error of the bootstrap means\n",
    "mean_bootstrap_means = np.mean(bootstrap_means)\n",
    "std_error_bootstrap_means = np.std(bootstrap_means, ddof=1) / np.sqrt(num_bootstrap_iterations)\n",
    "\n",
    "# Print the results\n",
    "print(\"Mean of bootstrap means:\", mean_bootstrap_means)\n",
    "print(\"Standard error of bootstrap means:\", std_error_bootstrap_means)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "27oUfZi-U1qp"
   },
   "source": [
    "(d) Comment on the estimated standard errors obtained using the `sm.GLM()` function and using the bootstrap."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **It is really important to understand the diff between boot_se and df(coef).std!! see notes above**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yf_msA6YU6O1"
   },
   "source": [
    "## 5.7.\n",
    "In Sections 5.1.2 and 5.1.3, we saw that the `cross_validate()` function can be used in order to compute the LOOCV test error estimate. Alternatively, one could compute those quantities using just `sm.GLM()` and the `predict()` method of the fitted model within a for loop. You will now take this approach in order to compute the LOOCV error for a simple logistic regression model on the `Weekly` data set. Recall that in the context of classification problems, the LOOCV error is given in (5.4).\n",
    "\n",
    "(a) Fit a logistic regression model that predicts `Direction` using `Lag1 and Lag2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Lag1</th>\n",
       "      <th>Lag2</th>\n",
       "      <th>Lag3</th>\n",
       "      <th>Lag4</th>\n",
       "      <th>Lag5</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Today</th>\n",
       "      <th>Direction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1990</td>\n",
       "      <td>0.816</td>\n",
       "      <td>1.572</td>\n",
       "      <td>-3.936</td>\n",
       "      <td>-0.229</td>\n",
       "      <td>-3.484</td>\n",
       "      <td>0.154976</td>\n",
       "      <td>-0.270</td>\n",
       "      <td>Down</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1990</td>\n",
       "      <td>-0.270</td>\n",
       "      <td>0.816</td>\n",
       "      <td>1.572</td>\n",
       "      <td>-3.936</td>\n",
       "      <td>-0.229</td>\n",
       "      <td>0.148574</td>\n",
       "      <td>-2.576</td>\n",
       "      <td>Down</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Year   Lag1   Lag2   Lag3   Lag4   Lag5    Volume  Today Direction\n",
       "0  1990  0.816  1.572 -3.936 -0.229 -3.484  0.154976 -0.270      Down\n",
       "1  1990 -0.270  0.816  1.572 -3.936 -0.229  0.148574 -2.576      Down"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('Weekly.csv')\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "6Uwy8wjPVQQ9"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Generalized Linear Model Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>   <td>['Direction[Down]', 'Direction[Up]']</td> <th>  No. Observations:  </th>  <td>  1089</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                            <td>GLM</td>                 <th>  Df Residuals:      </th>  <td>  1086</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model Family:</th>                  <td>Binomial</td>               <th>  Df Model:          </th>  <td>     2</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Link Function:</th>                   <td>Logit</td>                <th>  Scale:             </th> <td>  1.0000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>                          <td>IRLS</td>                 <th>  Log-Likelihood:    </th> <td> -744.11</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>                      <td>Tue, 07 May 2024</td>           <th>  Deviance:          </th> <td>  1488.2</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                          <td>13:48:53</td>               <th>  Pearson chi2:      </th> <td>1.09e+03</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Iterations:</th>                    <td>4</td>                  <th>  Pseudo R-squ. (CS):</th> <td>0.007303</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>               <td>nonrobust</td>              <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>         <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th> <td>   -0.2212</td> <td>    0.061</td> <td>   -3.599</td> <td> 0.000</td> <td>   -0.342</td> <td>   -0.101</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Lag1</th>      <td>    0.0387</td> <td>    0.026</td> <td>    1.477</td> <td> 0.140</td> <td>   -0.013</td> <td>    0.090</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Lag2</th>      <td>   -0.0602</td> <td>    0.027</td> <td>   -2.270</td> <td> 0.023</td> <td>   -0.112</td> <td>   -0.008</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Dep. Variable:}   & ['Direction[Down]', 'Direction[Up]'] & \\textbf{  No. Observations:  } &     1089    \\\\\n",
       "\\textbf{Model:}           &                 GLM                  & \\textbf{  Df Residuals:      } &     1086    \\\\\n",
       "\\textbf{Model Family:}    &               Binomial               & \\textbf{  Df Model:          } &        2    \\\\\n",
       "\\textbf{Link Function:}   &                Logit                 & \\textbf{  Scale:             } &    1.0000   \\\\\n",
       "\\textbf{Method:}          &                 IRLS                 & \\textbf{  Log-Likelihood:    } &   -744.11   \\\\\n",
       "\\textbf{Date:}            &           Tue, 07 May 2024           & \\textbf{  Deviance:          } &    1488.2   \\\\\n",
       "\\textbf{Time:}            &               13:48:53               & \\textbf{  Pearson chi2:      } &  1.09e+03   \\\\\n",
       "\\textbf{No. Iterations:}  &                  4                   & \\textbf{  Pseudo R-squ. (CS):} &  0.007303   \\\\\n",
       "\\textbf{Covariance Type:} &              nonrobust               & \\textbf{                     } &             \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lcccccc}\n",
       "                   & \\textbf{coef} & \\textbf{std err} & \\textbf{z} & \\textbf{P$> |$z$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n",
       "\\midrule\n",
       "\\textbf{Intercept} &      -0.2212  &        0.061     &    -3.599  &         0.000        &       -0.342    &       -0.101     \\\\\n",
       "\\textbf{Lag1}      &       0.0387  &        0.026     &     1.477  &         0.140        &       -0.013    &        0.090     \\\\\n",
       "\\textbf{Lag2}      &      -0.0602  &        0.027     &    -2.270  &         0.023        &       -0.112    &       -0.008     \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "%\\caption{Generalized Linear Model Regression Results}\n",
       "\\end{center}"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                          Generalized Linear Model Regression Results                           \n",
       "================================================================================================\n",
       "Dep. Variable:     ['Direction[Down]', 'Direction[Up]']   No. Observations:                 1089\n",
       "Model:                                              GLM   Df Residuals:                     1086\n",
       "Model Family:                                  Binomial   Df Model:                            2\n",
       "Link Function:                                    Logit   Scale:                          1.0000\n",
       "Method:                                            IRLS   Log-Likelihood:                -744.11\n",
       "Date:                                  Tue, 07 May 2024   Deviance:                       1488.2\n",
       "Time:                                          13:48:53   Pearson chi2:                 1.09e+03\n",
       "No. Iterations:                                       4   Pseudo R-squ. (CS):           0.007303\n",
       "Covariance Type:                              nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "Intercept     -0.2212      0.061     -3.599      0.000      -0.342      -0.101\n",
       "Lag1           0.0387      0.026      1.477      0.140      -0.013       0.090\n",
       "Lag2          -0.0602      0.027     -2.270      0.023      -0.112      -0.008\n",
       "==============================================================================\n",
       "\"\"\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#x_cols = ['Lag1','Lag2']\n",
    "#X = df[x_cols]\n",
    "#y = df['Direction']\n",
    "res = smf.glm(formula='Direction ~ Lag1 + Lag2', data=df, family=sm.families.Binomial()).fit()\n",
    "res.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FoTH8rTqVQjO"
   },
   "source": [
    "(b) Fit a logistic regression model that predicts `Direction` using `Lag1 and Lag2` using all but the first observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "6XWsNVqmVVDe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1089, 9) (1088, 9)\n"
     ]
    }
   ],
   "source": [
    "df1 = df.copy()\n",
    "df1 = df.iloc[1:,:]\n",
    "print(df.shape,df1.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Generalized Linear Model Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>   <td>['Direction[Down]', 'Direction[Up]']</td> <th>  No. Observations:  </th>  <td>  1088</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                            <td>GLM</td>                 <th>  Df Residuals:      </th>  <td>  1085</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model Family:</th>                  <td>Binomial</td>               <th>  Df Model:          </th>  <td>     2</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Link Function:</th>                   <td>Logit</td>                <th>  Scale:             </th> <td>  1.0000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>                          <td>IRLS</td>                 <th>  Log-Likelihood:    </th> <td> -743.26</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>                      <td>Tue, 07 May 2024</td>           <th>  Deviance:          </th> <td>  1486.5</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                          <td>13:52:53</td>               <th>  Pearson chi2:      </th> <td>1.09e+03</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Iterations:</th>                    <td>4</td>                  <th>  Pseudo R-squ. (CS):</th> <td>0.007373</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>               <td>nonrobust</td>              <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>         <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th> <td>   -0.2232</td> <td>    0.061</td> <td>   -3.630</td> <td> 0.000</td> <td>   -0.344</td> <td>   -0.103</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Lag1</th>      <td>    0.0384</td> <td>    0.026</td> <td>    1.466</td> <td> 0.143</td> <td>   -0.013</td> <td>    0.090</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Lag2</th>      <td>   -0.0608</td> <td>    0.027</td> <td>   -2.291</td> <td> 0.022</td> <td>   -0.113</td> <td>   -0.009</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Dep. Variable:}   & ['Direction[Down]', 'Direction[Up]'] & \\textbf{  No. Observations:  } &     1088    \\\\\n",
       "\\textbf{Model:}           &                 GLM                  & \\textbf{  Df Residuals:      } &     1085    \\\\\n",
       "\\textbf{Model Family:}    &               Binomial               & \\textbf{  Df Model:          } &        2    \\\\\n",
       "\\textbf{Link Function:}   &                Logit                 & \\textbf{  Scale:             } &    1.0000   \\\\\n",
       "\\textbf{Method:}          &                 IRLS                 & \\textbf{  Log-Likelihood:    } &   -743.26   \\\\\n",
       "\\textbf{Date:}            &           Tue, 07 May 2024           & \\textbf{  Deviance:          } &    1486.5   \\\\\n",
       "\\textbf{Time:}            &               13:52:53               & \\textbf{  Pearson chi2:      } &  1.09e+03   \\\\\n",
       "\\textbf{No. Iterations:}  &                  4                   & \\textbf{  Pseudo R-squ. (CS):} &  0.007373   \\\\\n",
       "\\textbf{Covariance Type:} &              nonrobust               & \\textbf{                     } &             \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lcccccc}\n",
       "                   & \\textbf{coef} & \\textbf{std err} & \\textbf{z} & \\textbf{P$> |$z$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n",
       "\\midrule\n",
       "\\textbf{Intercept} &      -0.2232  &        0.061     &    -3.630  &         0.000        &       -0.344    &       -0.103     \\\\\n",
       "\\textbf{Lag1}      &       0.0384  &        0.026     &     1.466  &         0.143        &       -0.013    &        0.090     \\\\\n",
       "\\textbf{Lag2}      &      -0.0608  &        0.027     &    -2.291  &         0.022        &       -0.113    &       -0.009     \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "%\\caption{Generalized Linear Model Regression Results}\n",
       "\\end{center}"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                          Generalized Linear Model Regression Results                           \n",
       "================================================================================================\n",
       "Dep. Variable:     ['Direction[Down]', 'Direction[Up]']   No. Observations:                 1088\n",
       "Model:                                              GLM   Df Residuals:                     1085\n",
       "Model Family:                                  Binomial   Df Model:                            2\n",
       "Link Function:                                    Logit   Scale:                          1.0000\n",
       "Method:                                            IRLS   Log-Likelihood:                -743.26\n",
       "Date:                                  Tue, 07 May 2024   Deviance:                       1486.5\n",
       "Time:                                          13:52:53   Pearson chi2:                 1.09e+03\n",
       "No. Iterations:                                       4   Pseudo R-squ. (CS):           0.007373\n",
       "Covariance Type:                              nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "Intercept     -0.2232      0.061     -3.630      0.000      -0.344      -0.103\n",
       "Lag1           0.0384      0.026      1.466      0.143      -0.013       0.090\n",
       "Lag2          -0.0608      0.027     -2.291      0.022      -0.113      -0.009\n",
       "==============================================================================\n",
       "\"\"\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res1 = smf.glm(formula = 'Direction ~ Lag1 + Lag2', data = df1, family = sm.families.Binomial()).fit()\n",
    "res1.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DDp-XqXjVVaT"
   },
   "source": [
    "(c) Use the model from (b) to predict the direction of the first observation. You can do this by predicting that the first observation will go up if $P(Direction = \"Up\"|Lag1, Lag2) > 0.5$. Was this observation correctly classified?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "Fsjn8YK4Vzry"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    0.429652\n",
       "dtype: float64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test = df[['Lag1', 'Lag2']].iloc[1,:]\n",
    "p_pred = res1.predict(x_test)\n",
    "p_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/b4/dg1qfb_d7dlfv97__53drbzr0000gn/T/ipykernel_33963/33669520.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Up'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mp_pred\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'Down'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1525\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mfinal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__nonzero__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mNoReturn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1527\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m   1528\u001b[0m             \u001b[0;34mf\"The truth value of a {type(self).__name__} is ambiguous. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m             \u001b[0;34m\"Use a.empty, a.bool(), a.item(), a.any() or a.all().\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1530\u001b[0m         )\n",
      "\u001b[0;31mValueError\u001b[0m: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all()."
     ]
    }
   ],
   "source": [
    "y_pred = 'Up' if (p_pred >= 0.5) else 'Down'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### !!!**warning**: \n",
    "The error message \"The truth value of a Series is ambiguous\" typically occurs when you're trying to use a boolean condition (e.g., p_pred >= 0.5) directly on a pandas Series object. This happens because pandas Series objects support element-wise operations, but they don't support direct boolean evaluation for the entire Series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_pred = (p_pred >= 0.5).astype(int)\n",
    "p_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_pred == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    Down\n",
       "dtype: object"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = p_pred.apply(lambda x: 'Up' if x == 1 else 'Down')\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9G-QtI9AVz_3"
   },
   "source": [
    "(d) Write a for loop from $i=1$ to $i=n$,where $n$ is the number of observations in the data set, that performs each of the following steps:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y3awSsK5WE87"
   },
   "source": [
    "i. Fit a logistic regression model using all but the ith observation to predict `Direction` using `Lag1` and `Lag2`.\n",
    "\n",
    "ii. Compute the posterior probability of the market moving up for the $i$th observation.\n",
    "\n",
    "iii. Use the posterior probability for the ith observation in order to predict whether or not the market moves up.\n",
    "\n",
    "iv. Determine whether or not an error was made in predicting the direction for the $i$th observation. If an error was made, then indicate this as a 1, and otherwise indicate it as a 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Direction    True\n",
       "Name: 0, dtype: bool"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['Direction']].iloc[i,:] == 'Down'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "id": "GJ7OYkiXWChu"
   },
   "outputs": [],
   "source": [
    "error = []\n",
    "for i in range(df.shape[0]):\n",
    "    df1 = df.drop(index =i)\n",
    "    res = smf.glm(formula = 'Direction ~ Lag1 + Lag2', data= df1, family = sm.families.Binomial()).fit()\n",
    "    p_pred = res.predict(df[['Lag1', 'Lag2']].iloc[i,:])\n",
    "    y_pred = p_pred.apply(lambda x : 'Up' if x > 0.5 else 'Down')\n",
    "    error.append(y_pred.values != df[['Direction']].iloc[i,:].values)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    0.428608\n",
      "dtype: float64\n",
      "['Down']\n",
      "1    0.429661\n",
      "dtype: float64\n",
      "['Down']\n",
      "2    0.425355\n",
      "dtype: float64\n",
      "['Down']\n"
     ]
    }
   ],
   "source": [
    "## this is the test block\n",
    "error1 = []\n",
    "for i in range(3):\n",
    "    df1 = df.drop(index =i)\n",
    "    res = smf.glm(formula = 'Direction ~ Lag1 + Lag2', data= df1, family = sm.families.Binomial()).fit()\n",
    "    p_pred = res.predict(df[['Lag1', 'Lag2']].iloc[i,:])\n",
    "    print(p_pred)\n",
    "    y_pred = p_pred.apply(lambda x : 'Up' if x > 0.5 else 'Down')\n",
    "    print(y_pred.values)\n",
    "    error1.append(y_pred.values != df[['Direction']].iloc[i,:].values)\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([False]), array([False]), array([ True])]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## this is the test block\n",
    "error1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Down'], dtype=object)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## this is the test block\n",
    "y_pred.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Down'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## this is the test block\n",
    "y_pred =  'Up' if p_pred.values > 0.5 else 'Down'\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1wtasfC3WUhp"
   },
   "source": [
    "(e) Take the average of the $n$ numbers obtained in (d)iv in order to obtain the LOOCV estimate for the test error. Comment on the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "id": "W_BgxEyBWYfp"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5500459136822773"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(error).astype(int).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mPZ9ER5DWn7P"
   },
   "source": [
    "## 5.8.\n",
    "We will now perform cross-validation on a simulated data set.\n",
    "\n",
    "(a) Generate a simulated data set as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "id": "PHIw4ALOWtHV"
   },
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(1)\n",
    "x = rng.normal(size=100)\n",
    "y = x - 2 * x**2 + rng.normal(size=100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4lPipvNdWxeI"
   },
   "source": [
    "In this data set, what is $n$ and what is $p$? Write out the model used to generate the data in equation form."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dhIjB-HOW4oX"
   },
   "source": [
    "\n",
    "n = number of observations = 100\n",
    "p = number of predictors\n",
    "\n",
    "$$\n",
    "y = x + 2x^{2} + \\epsilon\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xgtrOpcwW5Aa"
   },
   "source": [
    "(b) Create a scatterplot of X against Y . Comment on what you find."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "id": "WutSsS_iW77w"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x2848dc290>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2GuySbExW-f1"
   },
   "source": [
    "(c) Set a random seed, and then compute the LOOCV errors that\n",
    "result from fitting the following four models using least squares:\n",
    "$$\n",
    "i. Y = _0 + _1X + \n",
    "$$\n",
    "$$\n",
    "ii. Y =_0 +_1X+_2X^2 +\n",
    "$$\n",
    "$$\n",
    "iii. Y =_0 +_1X+_2X^2 +_3X^3 +\n",
    "$$\n",
    "$$\n",
    "iv. Y =_0 +_1X+_2X^2 +_3X^3 +_4X^4 +\n",
    "$$\n",
    "\n",
    "Note you may find it helpful to use the `data.frame()` function to create a single data set containing both X and Y ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "id": "P0qm0B8wXjeU"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y</th>\n",
       "      <th>x</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.544554</td>\n",
       "      <td>0.345584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.333950</td>\n",
       "      <td>0.821618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.013532</td>\n",
       "      <td>0.330437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-4.030442</td>\n",
       "      <td>-1.303157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.484861</td>\n",
       "      <td>0.905356</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          y         x\n",
       "0 -0.544554  0.345584\n",
       "1  0.333950  0.821618\n",
       "2 -0.013532  0.330437\n",
       "3 -4.030442 -1.303157\n",
       "4  0.484861  0.905356"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "random.seed(2)\n",
    "df1 = pd.DataFrame({'y':y, 'x':x})\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mse_scores:6.633029839181984, mae_scores:1.721636050669832\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "mse_scores = []\n",
    "mae_scores = []\n",
    "for i in range(df1.shape[0]):\n",
    "    df11 = df1.drop(index = i)\n",
    "    res = smf.ols(formula = 'y ~ x', data = df11).fit()\n",
    "\n",
    "    x_test = df1[['x']].iloc[i,:]\n",
    "    y_test = df1[['y']].iloc[i,:]\n",
    "    # Predict on test data\n",
    "    y_pred = res.predict(x_test)\n",
    "\n",
    "    # Compute MSE and MAE\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "    mse_scores.append(mse)\n",
    "    mae_scores.append(mae)\n",
    "# Mean Squared Error (MSE)/Mean Absolute Error (MAE):\n",
    "avg_mse = pd.Series(mse_scores).mean()\n",
    "avg_mae = pd.Series(mae_scores).mean()\n",
    "print('mse_scores:{}, mae_scores:{}'.format(avg_mse,avg_mae))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y</th>\n",
       "      <th>x</th>\n",
       "      <th>x2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.544554</td>\n",
       "      <td>0.345584</td>\n",
       "      <td>0.119428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.333950</td>\n",
       "      <td>0.821618</td>\n",
       "      <td>0.675056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.013532</td>\n",
       "      <td>0.330437</td>\n",
       "      <td>0.109189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-4.030442</td>\n",
       "      <td>-1.303157</td>\n",
       "      <td>1.698219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.484861</td>\n",
       "      <td>0.905356</td>\n",
       "      <td>0.819669</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          y         x        x2\n",
       "0 -0.544554  0.345584  0.119428\n",
       "1  0.333950  0.821618  0.675056\n",
       "2 -0.013532  0.330437  0.109189\n",
       "3 -4.030442 -1.303157  1.698219\n",
       "4  0.484861  0.905356  0.819669"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = pd.DataFrame({'y':y, 'x':x,'x2':x**2})\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mse_scores:1.1229368563419688, mae_scores:0.8088369470253669\n"
     ]
    }
   ],
   "source": [
    "mse_scores = []\n",
    "mae_scores = []\n",
    "for i in range(df2.shape[0]):\n",
    "    df21 = df2.drop(index = i)\n",
    "    res = smf.ols(formula = 'y ~ x + x2', data = df21).fit()\n",
    "\n",
    "    x_test = df2[['x','x2']].iloc[i,:]\n",
    "    y_test = df2[['y']].iloc[i,:]\n",
    "    # Predict on test data\n",
    "    y_pred = res.predict(x_test)\n",
    "\n",
    "    # Compute MSE and MAE\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "    mse_scores.append(mse)\n",
    "    mae_scores.append(mae)\n",
    "# Mean Squared Error (MSE)/Mean Absolute Error (MAE):\n",
    "avg_mse = pd.Series(mse_scores).mean()\n",
    "avg_mae = pd.Series(mae_scores).mean()\n",
    "print('mse_scores:{}, mae_scores:{}'.format(avg_mse,avg_mae)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y</th>\n",
       "      <th>x</th>\n",
       "      <th>x2</th>\n",
       "      <th>x3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.544554</td>\n",
       "      <td>0.345584</td>\n",
       "      <td>0.119428</td>\n",
       "      <td>0.041273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.333950</td>\n",
       "      <td>0.821618</td>\n",
       "      <td>0.675056</td>\n",
       "      <td>0.554639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.013532</td>\n",
       "      <td>0.330437</td>\n",
       "      <td>0.109189</td>\n",
       "      <td>0.036080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-4.030442</td>\n",
       "      <td>-1.303157</td>\n",
       "      <td>1.698219</td>\n",
       "      <td>-2.213046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.484861</td>\n",
       "      <td>0.905356</td>\n",
       "      <td>0.819669</td>\n",
       "      <td>0.742092</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          y         x        x2        x3\n",
       "0 -0.544554  0.345584  0.119428  0.041273\n",
       "1  0.333950  0.821618  0.675056  0.554639\n",
       "2 -0.013532  0.330437  0.109189  0.036080\n",
       "3 -4.030442 -1.303157  1.698219 -2.213046\n",
       "4  0.484861  0.905356  0.819669  0.742092"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3 = pd.DataFrame({'y':y, 'x':x,'x2':x**2,'x3':x**3})\n",
    "df3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mse_scores:1.301796548935887, mae_scores:0.8401313730476941\n"
     ]
    }
   ],
   "source": [
    "mse_scores = []\n",
    "mae_scores = []\n",
    "for i in range(df3.shape[0]):\n",
    "    df31 = df3.drop(index = i)\n",
    "    res = smf.ols(formula = 'y ~ x + x2+x3', data = df31).fit()\n",
    "\n",
    "    x_test = df3[['x','x2','x3']].iloc[i,:]\n",
    "    y_test = df3[['y']].iloc[i,:]\n",
    "    # Predict on test data\n",
    "    y_pred = res.predict(x_test)\n",
    "\n",
    "    # Compute MSE and MAE\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "    mse_scores.append(mse)\n",
    "    mae_scores.append(mae)\n",
    "# Mean Squared Error (MSE)/Mean Absolute Error (MAE):\n",
    "avg_mse = pd.Series(mse_scores).mean()\n",
    "avg_mae = pd.Series(mae_scores).mean()\n",
    "print('mse_scores:{}, mae_scores:{}'.format(avg_mse,avg_mae)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y</th>\n",
       "      <th>x</th>\n",
       "      <th>x2</th>\n",
       "      <th>x3</th>\n",
       "      <th>x4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.544554</td>\n",
       "      <td>0.345584</td>\n",
       "      <td>0.119428</td>\n",
       "      <td>0.041273</td>\n",
       "      <td>0.014263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.333950</td>\n",
       "      <td>0.821618</td>\n",
       "      <td>0.675056</td>\n",
       "      <td>0.554639</td>\n",
       "      <td>0.455701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.013532</td>\n",
       "      <td>0.330437</td>\n",
       "      <td>0.109189</td>\n",
       "      <td>0.036080</td>\n",
       "      <td>0.011922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-4.030442</td>\n",
       "      <td>-1.303157</td>\n",
       "      <td>1.698219</td>\n",
       "      <td>-2.213046</td>\n",
       "      <td>2.883947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.484861</td>\n",
       "      <td>0.905356</td>\n",
       "      <td>0.819669</td>\n",
       "      <td>0.742092</td>\n",
       "      <td>0.671858</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          y         x        x2        x3        x4\n",
       "0 -0.544554  0.345584  0.119428  0.041273  0.014263\n",
       "1  0.333950  0.821618  0.675056  0.554639  0.455701\n",
       "2 -0.013532  0.330437  0.109189  0.036080  0.011922\n",
       "3 -4.030442 -1.303157  1.698219 -2.213046  2.883947\n",
       "4  0.484861  0.905356  0.819669  0.742092  0.671858"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df4 = pd.DataFrame({'y':y, 'x':x,'x2':x**2,'x3':x**3,'x4':x**4})\n",
    "df4.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mse_scores:1.332394269417932, mae_scores:0.8342976582166507\n"
     ]
    }
   ],
   "source": [
    "mse_scores = []\n",
    "mae_scores = []\n",
    "for i in range(df4.shape[0]):\n",
    "    df41 = df4.drop(index = i)\n",
    "    res = smf.ols(formula = 'y ~ x + x2+x3+x4', data = df41).fit()\n",
    "\n",
    "    x_test = df4[['x','x2','x3','x4']].iloc[i,:]\n",
    "    y_test = df4[['y']].iloc[i,:]\n",
    "    # Predict on test data\n",
    "    y_pred = res.predict(x_test)\n",
    "\n",
    "    # Compute MSE and MAE\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "    mse_scores.append(mse)\n",
    "    mae_scores.append(mae)\n",
    "# Mean Squared Error (MSE)/Mean Absolute Error (MAE):\n",
    "avg_mse = pd.Series(mse_scores).mean()\n",
    "avg_mae = pd.Series(mae_scores).mean()\n",
    "print('mse_scores:{}, mae_scores:{}'.format(avg_mse,avg_mae)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qRh-UYsqXlna"
   },
   "source": [
    "(d) Repeat (c) using another random seed, and report your results. Are your results the same as what you got in (c)? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PAzdUJuXXmuJ"
   },
   "source": [
    "#### Answer:\n",
    "The results are exactly the same because we only remove one observation from the training set. Thus, there is no random effect resulting from the observations used for the test set. LOOCV will always be the same, no matter the random seed.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iO3kxEfYXnAG"
   },
   "source": [
    "(e) Which of the models in (c) had the smallest LOOCV error? Is this what you expected? Explain your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E3Bpvr6fXo1k"
   },
   "source": [
    "with x2, yes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YCYsdfdhXpHN"
   },
   "source": [
    "(f) Comment on the statistical significance of the coefficient estimates that results from fitting each of the models in (c) using least squares. Do these results agree with the conclusions drawn based on the cross-validation results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared:         </th> <td>   0.318</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.311</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   45.60</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Tue, 07 May 2024</td> <th>  Prob (F-statistic):</th> <td>1.04e-09</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>15:13:54</td>     <th>  Log-Likelihood:    </th> <td> -230.83</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   100</td>      <th>  AIC:               </th> <td>   465.7</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>    98</td>      <th>  BIC:               </th> <td>   470.9</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     1</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th> <td>   -1.4650</td> <td>    0.247</td> <td>   -5.937</td> <td> 0.000</td> <td>   -1.955</td> <td>   -0.975</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x</th>         <td>    1.9494</td> <td>    0.289</td> <td>    6.752</td> <td> 0.000</td> <td>    1.376</td> <td>    2.522</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>52.788</td> <th>  Durbin-Watson:     </th> <td>   1.972</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td> <th>  Jarque-Bera (JB):  </th> <td> 149.089</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td>-1.953</td> <th>  Prob(JB):          </th> <td>4.22e-33</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 7.530</td> <th>  Cond. No.          </th> <td>    1.20</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Dep. Variable:}    &        y         & \\textbf{  R-squared:         } &     0.318   \\\\\n",
       "\\textbf{Model:}            &       OLS        & \\textbf{  Adj. R-squared:    } &     0.311   \\\\\n",
       "\\textbf{Method:}           &  Least Squares   & \\textbf{  F-statistic:       } &     45.60   \\\\\n",
       "\\textbf{Date:}             & Tue, 07 May 2024 & \\textbf{  Prob (F-statistic):} &  1.04e-09   \\\\\n",
       "\\textbf{Time:}             &     15:13:54     & \\textbf{  Log-Likelihood:    } &   -230.83   \\\\\n",
       "\\textbf{No. Observations:} &         100      & \\textbf{  AIC:               } &     465.7   \\\\\n",
       "\\textbf{Df Residuals:}     &          98      & \\textbf{  BIC:               } &     470.9   \\\\\n",
       "\\textbf{Df Model:}         &           1      & \\textbf{                     } &             \\\\\n",
       "\\textbf{Covariance Type:}  &    nonrobust     & \\textbf{                     } &             \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lcccccc}\n",
       "                   & \\textbf{coef} & \\textbf{std err} & \\textbf{t} & \\textbf{P$> |$t$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n",
       "\\midrule\n",
       "\\textbf{Intercept} &      -1.4650  &        0.247     &    -5.937  &         0.000        &       -1.955    &       -0.975     \\\\\n",
       "\\textbf{x}         &       1.9494  &        0.289     &     6.752  &         0.000        &        1.376    &        2.522     \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lclc}\n",
       "\\textbf{Omnibus:}       & 52.788 & \\textbf{  Durbin-Watson:     } &    1.972  \\\\\n",
       "\\textbf{Prob(Omnibus):} &  0.000 & \\textbf{  Jarque-Bera (JB):  } &  149.089  \\\\\n",
       "\\textbf{Skew:}          & -1.953 & \\textbf{  Prob(JB):          } & 4.22e-33  \\\\\n",
       "\\textbf{Kurtosis:}      &  7.530 & \\textbf{  Cond. No.          } &     1.20  \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "%\\caption{OLS Regression Results}\n",
       "\\end{center}\n",
       "\n",
       "Notes: \\newline\n",
       " [1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                      y   R-squared:                       0.318\n",
       "Model:                            OLS   Adj. R-squared:                  0.311\n",
       "Method:                 Least Squares   F-statistic:                     45.60\n",
       "Date:                Tue, 07 May 2024   Prob (F-statistic):           1.04e-09\n",
       "Time:                        15:13:54   Log-Likelihood:                -230.83\n",
       "No. Observations:                 100   AIC:                             465.7\n",
       "Df Residuals:                      98   BIC:                             470.9\n",
       "Df Model:                           1                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "Intercept     -1.4650      0.247     -5.937      0.000      -1.955      -0.975\n",
       "x              1.9494      0.289      6.752      0.000       1.376       2.522\n",
       "==============================================================================\n",
       "Omnibus:                       52.788   Durbin-Watson:                   1.972\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              149.089\n",
       "Skew:                          -1.953   Prob(JB):                     4.22e-33\n",
       "Kurtosis:                       7.530   Cond. No.                         1.20\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = smf.ols(formula = 'y ~ x ', data = df4).fit()\n",
    "res.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared:         </th> <td>   0.887</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.884</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   379.5</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Tue, 07 May 2024</td> <th>  Prob (F-statistic):</th> <td>1.36e-46</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>15:14:05</td>     <th>  Log-Likelihood:    </th> <td> -141.06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   100</td>      <th>  AIC:               </th> <td>   288.1</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>    97</td>      <th>  BIC:               </th> <td>   295.9</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     2</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th> <td>   -0.0728</td> <td>    0.119</td> <td>   -0.611</td> <td> 0.543</td> <td>   -0.309</td> <td>    0.164</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x</th>         <td>    0.9663</td> <td>    0.126</td> <td>    7.647</td> <td> 0.000</td> <td>    0.715</td> <td>    1.217</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x2</th>        <td>   -2.0047</td> <td>    0.091</td> <td>  -22.072</td> <td> 0.000</td> <td>   -2.185</td> <td>   -1.824</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td> 1.338</td> <th>  Durbin-Watson:     </th> <td>   2.197</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.512</td> <th>  Jarque-Bera (JB):  </th> <td>   0.814</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.119</td> <th>  Prob(JB):          </th> <td>   0.666</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 3.372</td> <th>  Cond. No.          </th> <td>    2.23</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Dep. Variable:}    &        y         & \\textbf{  R-squared:         } &     0.887   \\\\\n",
       "\\textbf{Model:}            &       OLS        & \\textbf{  Adj. R-squared:    } &     0.884   \\\\\n",
       "\\textbf{Method:}           &  Least Squares   & \\textbf{  F-statistic:       } &     379.5   \\\\\n",
       "\\textbf{Date:}             & Tue, 07 May 2024 & \\textbf{  Prob (F-statistic):} &  1.36e-46   \\\\\n",
       "\\textbf{Time:}             &     15:14:05     & \\textbf{  Log-Likelihood:    } &   -141.06   \\\\\n",
       "\\textbf{No. Observations:} &         100      & \\textbf{  AIC:               } &     288.1   \\\\\n",
       "\\textbf{Df Residuals:}     &          97      & \\textbf{  BIC:               } &     295.9   \\\\\n",
       "\\textbf{Df Model:}         &           2      & \\textbf{                     } &             \\\\\n",
       "\\textbf{Covariance Type:}  &    nonrobust     & \\textbf{                     } &             \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lcccccc}\n",
       "                   & \\textbf{coef} & \\textbf{std err} & \\textbf{t} & \\textbf{P$> |$t$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n",
       "\\midrule\n",
       "\\textbf{Intercept} &      -0.0728  &        0.119     &    -0.611  &         0.543        &       -0.309    &        0.164     \\\\\n",
       "\\textbf{x}         &       0.9663  &        0.126     &     7.647  &         0.000        &        0.715    &        1.217     \\\\\n",
       "\\textbf{x2}        &      -2.0047  &        0.091     &   -22.072  &         0.000        &       -2.185    &       -1.824     \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lclc}\n",
       "\\textbf{Omnibus:}       &  1.338 & \\textbf{  Durbin-Watson:     } &    2.197  \\\\\n",
       "\\textbf{Prob(Omnibus):} &  0.512 & \\textbf{  Jarque-Bera (JB):  } &    0.814  \\\\\n",
       "\\textbf{Skew:}          &  0.119 & \\textbf{  Prob(JB):          } &    0.666  \\\\\n",
       "\\textbf{Kurtosis:}      &  3.372 & \\textbf{  Cond. No.          } &     2.23  \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "%\\caption{OLS Regression Results}\n",
       "\\end{center}\n",
       "\n",
       "Notes: \\newline\n",
       " [1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                      y   R-squared:                       0.887\n",
       "Model:                            OLS   Adj. R-squared:                  0.884\n",
       "Method:                 Least Squares   F-statistic:                     379.5\n",
       "Date:                Tue, 07 May 2024   Prob (F-statistic):           1.36e-46\n",
       "Time:                        15:14:05   Log-Likelihood:                -141.06\n",
       "No. Observations:                 100   AIC:                             288.1\n",
       "Df Residuals:                      97   BIC:                             295.9\n",
       "Df Model:                           2                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "Intercept     -0.0728      0.119     -0.611      0.543      -0.309       0.164\n",
       "x              0.9663      0.126      7.647      0.000       0.715       1.217\n",
       "x2            -2.0047      0.091    -22.072      0.000      -2.185      -1.824\n",
       "==============================================================================\n",
       "Omnibus:                        1.338   Durbin-Watson:                   2.197\n",
       "Prob(Omnibus):                  0.512   Jarque-Bera (JB):                0.814\n",
       "Skew:                           0.119   Prob(JB):                        0.666\n",
       "Kurtosis:                       3.372   Cond. No.                         2.23\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = smf.ols(formula = 'y ~ x + x2', data = df4).fit()\n",
    "res.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared:         </th> <td>   0.888</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.885</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   253.8</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Tue, 07 May 2024</td> <th>  Prob (F-statistic):</th> <td>1.70e-45</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>15:14:17</td>     <th>  Log-Likelihood:    </th> <td> -140.47</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   100</td>      <th>  AIC:               </th> <td>   288.9</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>    96</td>      <th>  BIC:               </th> <td>   299.4</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     3</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th> <td>   -0.0572</td> <td>    0.120</td> <td>   -0.477</td> <td> 0.635</td> <td>   -0.295</td> <td>    0.181</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x</th>         <td>    1.1146</td> <td>    0.187</td> <td>    5.945</td> <td> 0.000</td> <td>    0.742</td> <td>    1.487</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x2</th>        <td>   -2.0471</td> <td>    0.099</td> <td>  -20.673</td> <td> 0.000</td> <td>   -2.244</td> <td>   -1.851</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x3</th>        <td>   -0.0643</td> <td>    0.060</td> <td>   -1.070</td> <td> 0.287</td> <td>   -0.184</td> <td>    0.055</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td> 0.845</td> <th>  Durbin-Watson:     </th> <td>   2.199</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.655</td> <th>  Jarque-Bera (JB):  </th> <td>   0.392</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.052</td> <th>  Prob(JB):          </th> <td>   0.822</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 3.289</td> <th>  Cond. No.          </th> <td>    5.95</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Dep. Variable:}    &        y         & \\textbf{  R-squared:         } &     0.888   \\\\\n",
       "\\textbf{Model:}            &       OLS        & \\textbf{  Adj. R-squared:    } &     0.885   \\\\\n",
       "\\textbf{Method:}           &  Least Squares   & \\textbf{  F-statistic:       } &     253.8   \\\\\n",
       "\\textbf{Date:}             & Tue, 07 May 2024 & \\textbf{  Prob (F-statistic):} &  1.70e-45   \\\\\n",
       "\\textbf{Time:}             &     15:14:17     & \\textbf{  Log-Likelihood:    } &   -140.47   \\\\\n",
       "\\textbf{No. Observations:} &         100      & \\textbf{  AIC:               } &     288.9   \\\\\n",
       "\\textbf{Df Residuals:}     &          96      & \\textbf{  BIC:               } &     299.4   \\\\\n",
       "\\textbf{Df Model:}         &           3      & \\textbf{                     } &             \\\\\n",
       "\\textbf{Covariance Type:}  &    nonrobust     & \\textbf{                     } &             \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lcccccc}\n",
       "                   & \\textbf{coef} & \\textbf{std err} & \\textbf{t} & \\textbf{P$> |$t$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n",
       "\\midrule\n",
       "\\textbf{Intercept} &      -0.0572  &        0.120     &    -0.477  &         0.635        &       -0.295    &        0.181     \\\\\n",
       "\\textbf{x}         &       1.1146  &        0.187     &     5.945  &         0.000        &        0.742    &        1.487     \\\\\n",
       "\\textbf{x2}        &      -2.0471  &        0.099     &   -20.673  &         0.000        &       -2.244    &       -1.851     \\\\\n",
       "\\textbf{x3}        &      -0.0643  &        0.060     &    -1.070  &         0.287        &       -0.184    &        0.055     \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lclc}\n",
       "\\textbf{Omnibus:}       &  0.845 & \\textbf{  Durbin-Watson:     } &    2.199  \\\\\n",
       "\\textbf{Prob(Omnibus):} &  0.655 & \\textbf{  Jarque-Bera (JB):  } &    0.392  \\\\\n",
       "\\textbf{Skew:}          &  0.052 & \\textbf{  Prob(JB):          } &    0.822  \\\\\n",
       "\\textbf{Kurtosis:}      &  3.289 & \\textbf{  Cond. No.          } &     5.95  \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "%\\caption{OLS Regression Results}\n",
       "\\end{center}\n",
       "\n",
       "Notes: \\newline\n",
       " [1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                      y   R-squared:                       0.888\n",
       "Model:                            OLS   Adj. R-squared:                  0.885\n",
       "Method:                 Least Squares   F-statistic:                     253.8\n",
       "Date:                Tue, 07 May 2024   Prob (F-statistic):           1.70e-45\n",
       "Time:                        15:14:17   Log-Likelihood:                -140.47\n",
       "No. Observations:                 100   AIC:                             288.9\n",
       "Df Residuals:                      96   BIC:                             299.4\n",
       "Df Model:                           3                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "Intercept     -0.0572      0.120     -0.477      0.635      -0.295       0.181\n",
       "x              1.1146      0.187      5.945      0.000       0.742       1.487\n",
       "x2            -2.0471      0.099    -20.673      0.000      -2.244      -1.851\n",
       "x3            -0.0643      0.060     -1.070      0.287      -0.184       0.055\n",
       "==============================================================================\n",
       "Omnibus:                        0.845   Durbin-Watson:                   2.199\n",
       "Prob(Omnibus):                  0.655   Jarque-Bera (JB):                0.392\n",
       "Skew:                           0.052   Prob(JB):                        0.822\n",
       "Kurtosis:                       3.289   Cond. No.                         5.95\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = smf.ols(formula = 'y ~ x + x2+x3', data = df4).fit()\n",
    "res.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared:         </th> <td>   0.894</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.890</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   200.2</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Tue, 07 May 2024</td> <th>  Prob (F-statistic):</th> <td>2.22e-45</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>15:14:29</td>     <th>  Log-Likelihood:    </th> <td> -137.74</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   100</td>      <th>  AIC:               </th> <td>   285.5</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>    95</td>      <th>  BIC:               </th> <td>   298.5</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     4</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th> <td>    0.1008</td> <td>    0.136</td> <td>    0.743</td> <td> 0.460</td> <td>   -0.169</td> <td>    0.370</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x</th>         <td>    0.9050</td> <td>    0.205</td> <td>    4.423</td> <td> 0.000</td> <td>    0.499</td> <td>    1.311</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x2</th>        <td>   -2.5059</td> <td>    0.221</td> <td>  -11.336</td> <td> 0.000</td> <td>   -2.945</td> <td>   -2.067</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x3</th>        <td>    0.0338</td> <td>    0.073</td> <td>    0.466</td> <td> 0.642</td> <td>   -0.110</td> <td>    0.178</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x4</th>        <td>    0.1042</td> <td>    0.045</td> <td>    2.309</td> <td> 0.023</td> <td>    0.015</td> <td>    0.194</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td> 2.476</td> <th>  Durbin-Watson:     </th> <td>   2.163</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.290</td> <th>  Jarque-Bera (JB):  </th> <td>   2.097</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.118</td> <th>  Prob(JB):          </th> <td>   0.351</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 3.669</td> <th>  Cond. No.          </th> <td>    19.9</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Dep. Variable:}    &        y         & \\textbf{  R-squared:         } &     0.894   \\\\\n",
       "\\textbf{Model:}            &       OLS        & \\textbf{  Adj. R-squared:    } &     0.890   \\\\\n",
       "\\textbf{Method:}           &  Least Squares   & \\textbf{  F-statistic:       } &     200.2   \\\\\n",
       "\\textbf{Date:}             & Tue, 07 May 2024 & \\textbf{  Prob (F-statistic):} &  2.22e-45   \\\\\n",
       "\\textbf{Time:}             &     15:14:29     & \\textbf{  Log-Likelihood:    } &   -137.74   \\\\\n",
       "\\textbf{No. Observations:} &         100      & \\textbf{  AIC:               } &     285.5   \\\\\n",
       "\\textbf{Df Residuals:}     &          95      & \\textbf{  BIC:               } &     298.5   \\\\\n",
       "\\textbf{Df Model:}         &           4      & \\textbf{                     } &             \\\\\n",
       "\\textbf{Covariance Type:}  &    nonrobust     & \\textbf{                     } &             \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lcccccc}\n",
       "                   & \\textbf{coef} & \\textbf{std err} & \\textbf{t} & \\textbf{P$> |$t$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n",
       "\\midrule\n",
       "\\textbf{Intercept} &       0.1008  &        0.136     &     0.743  &         0.460        &       -0.169    &        0.370     \\\\\n",
       "\\textbf{x}         &       0.9050  &        0.205     &     4.423  &         0.000        &        0.499    &        1.311     \\\\\n",
       "\\textbf{x2}        &      -2.5059  &        0.221     &   -11.336  &         0.000        &       -2.945    &       -2.067     \\\\\n",
       "\\textbf{x3}        &       0.0338  &        0.073     &     0.466  &         0.642        &       -0.110    &        0.178     \\\\\n",
       "\\textbf{x4}        &       0.1042  &        0.045     &     2.309  &         0.023        &        0.015    &        0.194     \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lclc}\n",
       "\\textbf{Omnibus:}       &  2.476 & \\textbf{  Durbin-Watson:     } &    2.163  \\\\\n",
       "\\textbf{Prob(Omnibus):} &  0.290 & \\textbf{  Jarque-Bera (JB):  } &    2.097  \\\\\n",
       "\\textbf{Skew:}          &  0.118 & \\textbf{  Prob(JB):          } &    0.351  \\\\\n",
       "\\textbf{Kurtosis:}      &  3.669 & \\textbf{  Cond. No.          } &     19.9  \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "%\\caption{OLS Regression Results}\n",
       "\\end{center}\n",
       "\n",
       "Notes: \\newline\n",
       " [1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                      y   R-squared:                       0.894\n",
       "Model:                            OLS   Adj. R-squared:                  0.890\n",
       "Method:                 Least Squares   F-statistic:                     200.2\n",
       "Date:                Tue, 07 May 2024   Prob (F-statistic):           2.22e-45\n",
       "Time:                        15:14:29   Log-Likelihood:                -137.74\n",
       "No. Observations:                 100   AIC:                             285.5\n",
       "Df Residuals:                      95   BIC:                             298.5\n",
       "Df Model:                           4                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "Intercept      0.1008      0.136      0.743      0.460      -0.169       0.370\n",
       "x              0.9050      0.205      4.423      0.000       0.499       1.311\n",
       "x2            -2.5059      0.221    -11.336      0.000      -2.945      -2.067\n",
       "x3             0.0338      0.073      0.466      0.642      -0.110       0.178\n",
       "x4             0.1042      0.045      2.309      0.023       0.015       0.194\n",
       "==============================================================================\n",
       "Omnibus:                        2.476   Durbin-Watson:                   2.163\n",
       "Prob(Omnibus):                  0.290   Jarque-Bera (JB):                2.097\n",
       "Skew:                           0.118   Prob(JB):                        0.351\n",
       "Kurtosis:                       3.669   Cond. No.                         19.9\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = smf.ols(formula = 'y ~ x + x2+x3+x4', data = df4).fit()\n",
    "res.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kw7Vb4nBXuAW"
   },
   "source": [
    "## 5.9.\n",
    "We will now consider the `Boston` housing data set, from the ISLP library.\n",
    "\n",
    "(a) Based on this data set, provide an estimate for the population mean of `medv`. Call this estimate $\\hat{}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>crim</th>\n",
       "      <th>zn</th>\n",
       "      <th>indus</th>\n",
       "      <th>chas</th>\n",
       "      <th>nox</th>\n",
       "      <th>rm</th>\n",
       "      <th>age</th>\n",
       "      <th>dis</th>\n",
       "      <th>rad</th>\n",
       "      <th>tax</th>\n",
       "      <th>ptratio</th>\n",
       "      <th>black</th>\n",
       "      <th>lstat</th>\n",
       "      <th>medv</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1</td>\n",
       "      <td>296</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "      <td>33.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "      <td>36.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      crim    zn  indus  chas    nox     rm   age     dis  rad  tax  ptratio  \\\n",
       "0  0.00632  18.0   2.31     0  0.538  6.575  65.2  4.0900    1  296     15.3   \n",
       "1  0.02731   0.0   7.07     0  0.469  6.421  78.9  4.9671    2  242     17.8   \n",
       "2  0.02729   0.0   7.07     0  0.469  7.185  61.1  4.9671    2  242     17.8   \n",
       "3  0.03237   0.0   2.18     0  0.458  6.998  45.8  6.0622    3  222     18.7   \n",
       "4  0.06905   0.0   2.18     0  0.458  7.147  54.2  6.0622    3  222     18.7   \n",
       "\n",
       "    black  lstat  medv  \n",
       "0  396.90   4.98  24.0  \n",
       "1  396.90   9.14  21.6  \n",
       "2  392.83   4.03  34.7  \n",
       "3  394.63   2.94  33.4  \n",
       "4  396.90   5.33  36.2  "
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('Boston.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "id": "Uk5fRJoLX474"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22.532806324110677"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "medv = df['medv']\n",
    "mu_hat = medv.mean()\n",
    "mu_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YIiE9MmBX7KT"
   },
   "source": [
    "(b) Provide an estimate of the standard error of $\\hat{}$. Interpret this result.\n",
    "\n",
    "$Hint$: We can compute the standard error of the sample mean by dividing the sample standard deviation by the square root of the number of observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "id": "44UdJG2bYFZ6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4088611474975351"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "se_mu = medv.std()/np.sqrt(len(medv))\n",
    "se_mu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mVg0ZOFDYFxW"
   },
   "source": [
    "(c) Now estimate the standard error of $\\hat{}$ using the bootstrap. How does this compare to your answer from (b)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "id": "srJidxlXYKKK"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of bootstrap means: 22.52969130434783\n",
      "Standard error of bootstrap means: 0.405526665018759\n"
     ]
    }
   ],
   "source": [
    "# Set the random seed for reproducibility\n",
    "np.random.seed(17)\n",
    "\n",
    "# Sample data\n",
    "original_data = medv\n",
    "\n",
    "# Number of bootstrap iterations\n",
    "num_bootstrap_iterations = 1000\n",
    "\n",
    "# Bootstrap resampling\n",
    "bootstrap_samples = []\n",
    "for _ in range(num_bootstrap_iterations):\n",
    "    # Generate random indices with replacement\n",
    "    sample = np.random.choice(original_data, size=len(original_data), replace=True)\n",
    "    # Get the bootstrap sample\n",
    "    bootstrap_sample = sample\n",
    "    # Append the bootstrap sample to the list\n",
    "    bootstrap_samples.append(bootstrap_sample)\n",
    "\n",
    "# Conduct analysis on the resampled data (e.g., compute statistics)\n",
    "# For example, calculate the mean of each bootstrap sample\n",
    "bootstrap_means = [sample.mean() for sample in bootstrap_samples]\n",
    "bootstrap_std = [sample.std() for sample in bootstrap_samples]\n",
    "\n",
    "# Compute the mean and standard error of the bootstrap means\n",
    "mean_bootstrap_means = np.mean(bootstrap_means)\n",
    "std_error_bootstrap_means = np.std(bootstrap_means,ddof = 1)\n",
    "\n",
    "# Print the results\n",
    "print(\"Mean of bootstrap means:\", mean_bootstrap_means)\n",
    "print(\"Standard error of bootstrap means:\", std_error_bootstrap_means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrap estimate of mean:  22.529328873517787\n",
      "Bootstrap estimate of std. error:  0.4096425744290966\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(312)\n",
    "n_bootstraps = 10000\n",
    "means = np.empty(n_bootstraps)\n",
    "for i in range(n_bootstraps):\n",
    "    sample = resample(medv)\n",
    "    means[i] = sample.mean()\n",
    "print(\"Bootstrap estimate of mean: \", means.mean())\n",
    "print(\"Bootstrap estimate of std. error: \", means.std(ddof = 1))\n",
    "##Notes: ddof=1: This parameter stands for \"Delta Degrees of Freedom\" and is set to 1, \n",
    "## indicating that Bessel's correction should be applied."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6B1cu5vUYMzb"
   },
   "source": [
    "(d) Based on your bootstrap estimate from (c), provide a 95% confidence interval for the mean of `medv`. Compare it to the results obtained by using `Boston['medv'].std()` and the two standard error rule (3.9).\n",
    "\n",
    "$Hint$: You can approximate a 95 % confidence interval using the formula $[\\hat{}  2SE(\\hat{}), \\hat{} + 2SE(\\hat{})]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "id": "VRDRnTwpYrXQ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21.710043724659595 23.34861402237598\n"
     ]
    }
   ],
   "source": [
    "mu_hat = means.mean()\n",
    "mu_se = means.std(ddof = 1)\n",
    "\n",
    "CI_lb = mu_hat - 2* mu_se\n",
    "CI_ub = mu_hat + 2* mu_se\n",
    "print(CI_lb, CI_ub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21.715084029115605 23.35052861910575\n"
     ]
    }
   ],
   "source": [
    "mu_hat = medv.mean()\n",
    "se_mu = medv.std()/np.sqrt(len(medv))\n",
    "CI_lb = mu_hat - 2* se_mu\n",
    "CI_ub = mu_hat + 2* se_mu\n",
    "print(CI_lb, CI_ub)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HquCSZvmYr1-"
   },
   "source": [
    "(e) Based on this data set, provide an estimate, $\\hat{}_{med}$, for the median value of `medv` in the population."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "id": "2bWDtH-CY46c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21.2"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "med_hat = medv.median()\n",
    "med_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zr51K7aIY7A5"
   },
   "source": [
    "(f)We now would like to estimate the standard error of$\\hat{}_{med}$. Unfortunately, there is no simple formula for computing the standard error of the median. Instead, estimate the standard error of the median using the bootstrap. Comment on your findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "id": "ivewbx4UZKmI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrap estimate of median:  21.179145\n",
      "Bootstrap estimate of median's std. error:  0.3803186600777302\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(312)\n",
    "n_bootstraps = 10000\n",
    "medians = np.empty(n_bootstraps)\n",
    "for i in range(n_bootstraps):\n",
    "    sample = resample(medv)\n",
    "    medians[i] = sample.median()\n",
    "print(\"Bootstrap estimate of median: \", medians.mean())\n",
    "print(\"Bootstrap estimate of median's std. error: \", medians.std(ddof = 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PPnH3z4nZMvs"
   },
   "source": [
    "(g) Based on this data set, provide an estimate for the tenth percentile of medv in Boston census tracts. Call this quantity $\\hat{}_{0.1}$. (You can use the `np.percentile()` function.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "id": "ow5oG27SZZsP"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12.75"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mu_10_hat = np.percentile(medv,10)\n",
    "mu_10_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UTBVkRHSZdSz"
   },
   "source": [
    "(h) Use the bootstrap to estimate the standard error of $\\hat{}_{0.1}$. Comment on your findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "id": "6HNnBQ0tZlMw"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrap estimate of median:  12.749675\n",
      "Bootstrap estimate of median's std. error:  0.5090614488751559\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(312)\n",
    "n_bootstraps = 10000\n",
    "mu_10 = np.empty(n_bootstraps)\n",
    "for i in range(n_bootstraps):\n",
    "    sample = resample(medv)\n",
    "    mu_10[i] = np.percentile(sample,10)\n",
    "print(\"Bootstrap estimate of median: \", mu_10.mean())\n",
    "print(\"Bootstrap estimate of median's std. error: \", mu_10.std(ddof = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMD83K4MWiDk1ibgiwrBR2X",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
